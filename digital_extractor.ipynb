{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Shift: The Evolution of Products and Platforms in Portuguese E-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Extraction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"www.fnac.pt\", \"www.worten.pt\", \"www.elcorteingles.pt\", \"www.radiopopular.pt\", \"www.staples.pt\", \"www.pcdiga.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para procurar no arquivo.pt\n",
    "def arquivo_search(query=None, max_items=500, from_year=None, to_year=None, site=None, doc_type=None, version_history_url=None):\n",
    "    if version_history_url:\n",
    "        encoded_url = urllib.parse.quote(version_history_url, safe='')\n",
    "        base_url = f\"https://arquivo.pt/textsearch?versionHistory={encoded_url}\"\n",
    "    else:\n",
    "        base_url = \"https://arquivo.pt/textsearch?q=\" + urllib.parse.quote(query)  \n",
    "    \n",
    "    if from_year and to_year:\n",
    "        base_url += f\"&from={from_year}&to={to_year}\"\n",
    "    \n",
    "    if site:\n",
    "        base_url += f\"&siteSearch={site}\"\n",
    "    \n",
    "    if doc_type:\n",
    "        base_url += f\"&type={doc_type}\"\n",
    "    \n",
    "    base_url += f\"&maxItems={max_items}&prettyPrint=false\"\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Erro na requisição: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting websites links from Arquivo.pt (2005 to 2023) for the following retailers:\n",
    "worten, fnac, radio popular, el corte ingles, pc diga, staples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair a pagina principal de cada um dos sites, ao longo dos anos (2004 a 2023)\n",
    "def process_sites(sites):\n",
    "    site_data = {}\n",
    "\n",
    "    for site in sites:\n",
    "        print(f\"Processando site: {site}\")\n",
    "        site_links_by_year = {year: None for year in range(2005, 2024)}\n",
    "        \n",
    "        for year in range(2005, 2024):\n",
    "            \n",
    "            #procura pelo version history\n",
    "            version_history_url = f\"http://{site}/\"\n",
    "            response = arquivo_search(version_history_url=version_history_url, from_year=year, to_year=year + 1)\n",
    "\n",
    "            if response and 'response_items' in response:\n",
    "                response_items = response['response_items']\n",
    "\n",
    "                for item in response_items:\n",
    "                    item_year = int(item['tstamp'][0:4]) # os primeiros 4 caracteres representam o ano\n",
    "\n",
    "                    if item_year == year and item['originalURL'] == version_history_url:\n",
    "                        if site_links_by_year[year] is None:\n",
    "                            site_links_by_year[year] = item['linkToArchive']\n",
    "                        else:\n",
    "                            break\n",
    "        \n",
    "        site_data[site] = site_links_by_year\n",
    "    \n",
    "    return site_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando site: www.fnac.pt\n",
      "Processando site: www.worten.pt\n",
      "Processando site: www.elcorteingles.pt\n",
      "Processando site: www.radiopopular.pt\n",
      "Processando site: www.staples.pt\n",
      "Processando site: www.pcdiga.com\n",
      "{'www.fnac.pt': {2005: None, 2006: 'https://arquivo.pt/wayback/20061118120805/http://www.fnac.pt/', 2007: 'https://arquivo.pt/wayback/20070928223117/http://www.fnac.pt/', 2008: 'https://arquivo.pt/wayback/20081027081756/http://www.fnac.pt/', 2009: 'https://arquivo.pt/wayback/20091218064527/http://www.fnac.pt/', 2010: 'https://arquivo.pt/wayback/20100804062306/http://www.fnac.pt/', 2011: 'https://arquivo.pt/wayback/20110702090458/http://www.fnac.pt/', 2012: 'https://arquivo.pt/wayback/20120122102914/http://www.fnac.pt/', 2013: 'https://arquivo.pt/wayback/20131106231750/http://www.fnac.pt/', 2014: 'https://arquivo.pt/wayback/20141127075233/http://www.fnac.pt/', 2015: 'https://arquivo.pt/wayback/20151124075844/http://www.fnac.pt/', 2016: 'https://arquivo.pt/wayback/20161106090812/http://www.fnac.pt/', 2017: None, 2018: 'https://arquivo.pt/wayback/20181219181539/http://www.fnac.pt/', 2019: None, 2020: 'https://arquivo.pt/wayback/20201231205450/http://www.fnac.pt/', 2021: 'https://arquivo.pt/wayback/20211124235054/http://www.fnac.pt/', 2022: 'https://arquivo.pt/wayback/20221229233410/http://www.fnac.pt/', 2023: 'https://arquivo.pt/wayback/20230622161513/http://www.fnac.pt/'}, 'www.worten.pt': {2005: None, 2006: None, 2007: 'https://arquivo.pt/wayback/20070611190104/http://www.worten.pt/', 2008: 'https://arquivo.pt/wayback/20081022044251/http://www.worten.pt/', 2009: 'https://arquivo.pt/wayback/20091218174523/http://www.worten.pt/', 2010: 'https://arquivo.pt/wayback/20100803143044/http://www.worten.pt/', 2011: 'https://arquivo.pt/wayback/20110703082018/http://www.worten.pt/', 2012: 'https://arquivo.pt/wayback/20120123100556/http://www.worten.pt/', 2013: 'https://arquivo.pt/wayback/20131107114716/http://www.worten.pt/', 2014: 'https://arquivo.pt/wayback/20140930233937/http://www.worten.pt/', 2015: 'https://arquivo.pt/wayback/20151125065825/http://www.worten.pt/', 2016: 'https://arquivo.pt/wayback/20161107032031/http://www.worten.pt/', 2017: None, 2018: 'https://arquivo.pt/wayback/20181219190115/http://www.worten.pt/', 2019: None, 2020: 'https://arquivo.pt/wayback/20201231184214/http://www.worten.pt/', 2021: None, 2022: 'https://arquivo.pt/wayback/20220802112455/http://www.worten.pt/', 2023: 'https://arquivo.pt/wayback/20230123170609/http://www.worten.pt/'}, 'www.elcorteingles.pt': {2005: 'https://arquivo.pt/wayback/20050725031922/http://www.elcorteingles.pt/', 2006: 'https://arquivo.pt/wayback/20060216170739/http://www.elcorteingles.pt/', 2007: 'https://arquivo.pt/wayback/20070929080902/http://www.elcorteingles.pt/', 2008: 'https://arquivo.pt/wayback/20081021184312/http://www.elcorteingles.pt/', 2009: 'https://arquivo.pt/wayback/20091218054927/http://www.elcorteingles.pt/', 2010: 'https://arquivo.pt/wayback/20100804081831/http://www.elcorteingles.pt/', 2011: 'https://arquivo.pt/wayback/20110702070508/http://www.elcorteingles.pt/', 2012: 'https://arquivo.pt/wayback/20120122091210/http://www.elcorteingles.pt/', 2013: 'https://arquivo.pt/wayback/20131106212209/http://www.elcorteingles.pt/', 2014: 'https://arquivo.pt/wayback/20140930070204/http://www.elcorteingles.pt/', 2015: 'https://arquivo.pt/wayback/20151124053702/http://www.elcorteingles.pt/', 2016: 'https://arquivo.pt/wayback/20161106073144/http://www.elcorteingles.pt/', 2017: 'https://arquivo.pt/wayback/20171213035602/http://www.elcorteingles.pt/', 2018: None, 2019: None, 2020: 'https://arquivo.pt/wayback/20201231193929/http://www.elcorteingles.pt/', 2021: None, 2022: 'https://arquivo.pt/wayback/20221214020227/http://www.elcorteingles.pt/', 2023: 'https://arquivo.pt/wayback/20230302004226/http://www.elcorteingles.pt/'}, 'www.radiopopular.pt': {2005: 'https://arquivo.pt/wayback/20050722223614/http://www.radiopopular.pt/', 2006: None, 2007: 'https://arquivo.pt/wayback/20070929122436/http://www.radiopopular.pt/', 2008: 'https://arquivo.pt/wayback/20081022013802/http://www.radiopopular.pt/', 2009: 'https://arquivo.pt/wayback/20091218134419/http://www.radiopopular.pt/', 2010: 'https://arquivo.pt/wayback/20100803165209/http://www.radiopopular.pt/', 2011: 'https://arquivo.pt/wayback/20110703001054/http://www.radiopopular.pt/', 2012: 'https://arquivo.pt/wayback/20120123020522/http://www.radiopopular.pt/', 2013: 'https://arquivo.pt/wayback/20131107082040/http://www.radiopopular.pt/', 2014: 'https://arquivo.pt/wayback/20140930183354/http://www.radiopopular.pt/', 2015: 'https://arquivo.pt/wayback/20151124215123/http://www.radiopopular.pt/', 2016: 'https://arquivo.pt/wayback/20161106193800/http://www.radiopopular.pt/', 2017: 'https://arquivo.pt/wayback/20170222115920/http://www.radiopopular.pt/', 2018: 'https://arquivo.pt/wayback/20181015061743/http://www.radiopopular.pt/', 2019: 'https://arquivo.pt/wayback/20191119234250/http://www.radiopopular.pt/', 2020: 'https://arquivo.pt/wayback/20201014213146/http://www.radiopopular.pt/', 2021: None, 2022: 'https://arquivo.pt/wayback/20220802044650/http://www.radiopopular.pt/', 2023: 'https://arquivo.pt/wayback/20230123105349/http://www.radiopopular.pt/'}, 'www.staples.pt': {2005: 'https://arquivo.pt/wayback/20050722174753/http://www.staples.pt/', 2006: None, 2007: 'https://arquivo.pt/wayback/20070610185333/http://www.staples.pt/', 2008: 'https://arquivo.pt/wayback/20081022031557/http://www.staples.pt/', 2009: 'https://arquivo.pt/wayback/20091218154357/http://www.staples.pt/', 2010: 'https://arquivo.pt/wayback/20100608193358/http://www.staples.pt/', 2011: 'https://arquivo.pt/wayback/20110703041004/http://www.staples.pt/', 2012: 'https://arquivo.pt/wayback/20120123054704/http://www.staples.pt/', 2013: 'https://arquivo.pt/wayback/20131107101945/http://www.staples.pt/', 2014: 'https://arquivo.pt/wayback/20140930210106/http://www.staples.pt/', 2015: 'https://arquivo.pt/wayback/20151125004810/http://www.staples.pt/', 2016: 'https://arquivo.pt/wayback/20161106221541/http://www.staples.pt/', 2017: 'https://arquivo.pt/wayback/20170807081228/http://www.staples.pt/', 2018: 'https://arquivo.pt/wayback/20181101070404/http://www.staples.pt/', 2019: 'https://arquivo.pt/wayback/20191218131438/http://www.staples.pt/', 2020: 'https://arquivo.pt/wayback/20201231192426/http://www.staples.pt/', 2021: 'https://arquivo.pt/wayback/20211001222752/http://www.staples.pt/', 2022: 'https://arquivo.pt/wayback/20221014062758/http://www.staples.pt/', 2023: 'https://arquivo.pt/wayback/20230125024755/http://www.staples.pt/'}, 'www.pcdiga.com': {2005: 'https://arquivo.pt/wayback/20050719124815/http://www.pcdiga.com/', 2006: None, 2007: None, 2008: 'https://arquivo.pt/wayback/20081022130111/http://www.pcdiga.com/', 2009: 'https://arquivo.pt/wayback/20091219171727/http://www.pcdiga.com/', 2010: 'https://arquivo.pt/wayback/20100804133208/http://www.pcdiga.com/', 2011: 'https://arquivo.pt/wayback/20110122010135/http://www.pcdiga.com/', 2012: None, 2013: 'https://arquivo.pt/wayback/20131105233225/http://www.pcdiga.com/', 2014: 'https://arquivo.pt/wayback/20140905212632/http://www.pcdiga.com/', 2015: 'https://arquivo.pt/wayback/20151112163720/http://www.pcdiga.com/', 2016: 'https://arquivo.pt/wayback/20160204172024/http://www.pcdiga.com/', 2017: 'https://arquivo.pt/wayback/20170209162925/http://www.pcdiga.com/', 2018: 'https://arquivo.pt/wayback/20181101232309/http://www.pcdiga.com/', 2019: 'https://arquivo.pt/wayback/20191221210603/http://www.pcdiga.com/', 2020: 'https://arquivo.pt/wayback/20200607061806/http://www.pcdiga.com/', 2021: 'https://arquivo.pt/wayback/20210821121647/http://www.pcdiga.com/', 2022: 'https://arquivo.pt/wayback/20220423225206/http://www.pcdiga.com/', 2023: 'https://arquivo.pt/wayback/20230126013756/http://www.pcdiga.com/'}}\n"
     ]
    }
   ],
   "source": [
    "dados_sites = process_sites(sites)\n",
    "\n",
    "print(dados_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar os dados num ficheiro csv\n",
    "df = pd.DataFrame(dados_sites)\n",
    "df.to_csv(\"data/sites_links.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Extracting product categories from the websites (2007 | 2010 | 2015 | 2020 | 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['ano', 'link', 'site', 'numero_categorias', 'lista_categorias', 'dicionario_subcategorias']\n",
    "header_df = pd.DataFrame(columns=header)\n",
    "header_df.to_csv(\"ecommerce_category_analysis_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_2007_worten(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('ul', class_='menu')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_name = category.find('a').get_text(strip=True)\n",
    "            \n",
    "        # nao tem subcategorias\n",
    "            categories_dict[main_category_name] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        return 0, {}\n",
    "    \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year <= 2012:\n",
    "                link = link + 'Splash.aspx'\n",
    "            if year == 2007:\n",
    "                num_categories, category_dict = extractor_2007_worten(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='menu menu-category')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='sub'):\n",
    "                main_category_name = category.find('a', class_='label').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('a', class_='label').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            link = link + 'default.aspx'    \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', id='nav')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='level1'):\n",
    "                main_category_name = category.find('a').find('span').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='level2'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            # se houver subcategorias com [] é porque não tem subcategorias, e eliminar categoria\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values or site_column not in category_analysis['site'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_worten_2020_23(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='nav-sub js-nav-sub')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='nav-item nav-item-sub'):\n",
    "                \n",
    "                main_category_name = category.find('span', class_='nav-a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul', class_='nav nav-sub nav-sub-child js-nav-sub')\n",
    "                for sub in subcategories_container.find_all('li', class_='nav-item-sub'):\n",
    "                    label = sub.find('label', class_='nav-trigger js-nav-trigger')\n",
    "                    if label:\n",
    "                        subcategory_name = label.find('a', class_='nav-a')\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name.get_text(strip=True).lower())\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_worten_2020_23(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staples Extracted Categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_2007_10(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('span', id= lambda x: x and 'categorias' in x.lower())\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='tracos_centro'):\n",
    "            a_tag = div.find('a')\n",
    "            if not a_tag:\n",
    "                continue\n",
    "\n",
    "            category_name = a_tag.get_text(strip=True)\n",
    "\n",
    "            if 'tit_centro_blue_bold' in a_tag.get('class', []):\n",
    "                # Main category\n",
    "                current_main_category = category_name\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                # Subcategory\n",
    "                if current_main_category:\n",
    "                    categories_dict[current_main_category].append(\n",
    "                         category_name\n",
    "                    )\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'default.aspx'\n",
    "            if year == 2007 or year == 2010:\n",
    "                num_categories, category_dict = get_categories_staples_2007_10(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_staples_15(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_staples_20(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_23(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='container-menu-children')\n",
    "        print(categories_container)\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='children-cont'):\n",
    "            main_category_tag = div.find('span', class_='pr-name')\n",
    "            if main_category_tag:\n",
    "                current_main_category = main_category_tag.get_text(strip=True)\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                if current_main_category:\n",
    "                    subcategory_link = div.find('a', class_='menu-link_a')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.get_text(strip=True)\n",
    "                        categories_dict[current_main_category].append(subcategory_name)\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2023:\n",
    "                num_categories, category_dict = get_categories_staples_23(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PcDiga Extracted Categories:\n",
    "- náo é possivel extrair categorias de 2007 entao vamos extrair de 2008\n",
    "- nao consegui extrair categorias de 2023 nao conseguia aceder ao site assumi, que as diferencças de categorias entre 2020 e 2023 seriam minimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_08(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('map', attrs={'name': 'Map'})\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('area'):\n",
    "            # extrair o nome do href = ...?Familia=nome\n",
    "            category_href = category\n",
    "            main_category_name = category.get('href').split('=')[-1]\n",
    "\n",
    "            subcategories = []\n",
    "\n",
    "            # a categoria principal não tem subcategorias\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'pcdiga/'\n",
    "            if year == 2008 or year == 2010:\n",
    "                n_year = year - 1 if year == 2008 else year\n",
    "                num_categories, category_dict = get_categories_pcdiga_08(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': n_year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('div', id='masterdiv')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('table', class_='menu1'):\n",
    "            main_category_name = category.find('a', class_='menu').get_text(strip=True)\n",
    "            subcategories = []\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "\n",
    "                num_categories, category_dict = get_categories_pcdiga_15(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories = soup.find('div', class_='megamenu-wrapper')\n",
    "        categories_container = categories.find('ul', class_='megamenu')\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_link = category.find('a', class_='i-link')\n",
    "            svg_element = category.find('span', class_='svg-i svg-arrow')\n",
    "            if not main_category_link or not svg_element:\n",
    "                continue  \n",
    "\n",
    "            main_category_name = main_category_link.find('span', class_='lnk-text').get_text(strip=True)\n",
    "\n",
    "            subcategories = []\n",
    "            subcategories_container = category.find('div', class_='submenu')\n",
    "            if subcategories_container:\n",
    "                for subcategory in subcategories_container.find_all('li'):\n",
    "                    subcategory_link = subcategory.find('a', class_='i-link')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)  \n",
    "\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_pcdiga_20(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RadioPopular Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in soup.find_all('td', align='left'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.find('img').get('alt')\n",
    "                    categories_dict[main_category_name] = []\n",
    "            for i in list(categories_dict.keys()):\n",
    "                if i == 'Recrutamento':\n",
    "                    del categories_dict[i]\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010 or year == 2007:\n",
    "                num_categories, category_dict = get_categories_2010_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            div = soup.find('div', id='nav')\n",
    "            if div is None:\n",
    "                return 0, {} \n",
    "\n",
    "            for category in div.find_all('li', class_='dir'):\n",
    "                main_category_name = category.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='dir'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                if subcategories:\n",
    "                    categories_dict[main_category_name] = subcategories\n",
    "                else:\n",
    "                    categories_dict[main_category_name] = []\n",
    "\n",
    "            # vamos ver todas as keys que tem [] e eliminar, pois não tem subcategorias\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            ul = soup.find('ul', class_='categories')\n",
    "\n",
    "            for i in ul.find_all('li', class_='category link cb'):\n",
    "                main_category_name = i.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "                div = i.find('div', class_='subcategories')\n",
    "                if div:\n",
    "                    for subcategory in div.find_all('li', class_=\"subcategory family link\"):\n",
    "                        subcategory_name = subcategory.get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')    \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_2020_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o elcorteingles nao tem categorias de produtos, nao vamos utilizar este site para fazer esta comparação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fnac Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a nao ser prossivel extrair categorias de 2006 e 2008 nao fazemos comparação com esses anos na fnac\n",
    "\n",
    "**Erro**: ```O seu browser não aceita cookies, pelo que não é possível o acesso ao nosso site.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='menu')\n",
    "            u = div.find('ul', style=\"margin-left: 40px\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                ul = category.find('ul')\n",
    "                if ul:\n",
    "                    for subcategory in ul.find_all('li'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategory_name = subcategory_name.replace('»', '').replace('\\r\\n', '').strip()\n",
    "                        subcategory_name = ' '.join(subcategory_name.split())\n",
    "                        subcategories.append(subcategory_name)\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "            for value in categories_dict.values():\n",
    "                if 'Ver todos os produtos' in value:\n",
    "                    value.remove('Ver todos os produtos')\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='MENU')\n",
    "            u = div.find('ul', id=\"onglets\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    span = a.find('span', class_='inner')\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                \n",
    "                div = category.find('div', class_='megaMenu')\n",
    "                if div:\n",
    "                    for sub in div.find_all('dt'):\n",
    "                        # pode ja ter subcategoria ou ter um <a> com a subcategoria\n",
    "                        subcategory_name = sub.get_text(strip=True)\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name)\n",
    "\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='Sidebar-nano nano')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"CategoryNav js-CategoryNav\")\n",
    "            for il in ul.find_all('li', class_='CategoryNav-item js-CategoryNav-item'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_2020_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arquivo.pt/wayback/20230622161513mp_/http://www.fnac.pt/\n"
     ]
    }
   ],
   "source": [
    "def get_categories_2023_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='SideNavPanel-listWrapper')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"SideNavPanel-list\")\n",
    "            for il in ul.find_all('li', class_='SideNavPanel-listItem js-SideNavPanel-listItem'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2023:\n",
    "\n",
    "                num_categories, category_dict = get_categories_2023_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação  de diferentes preços em certos produtos em diferentes lojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coletar_dados_por_periodo(models, sites, periodos):\n",
    "    info = []\n",
    "\n",
    "    # Iterar sobre cada site e modelo para buscar informações no arquivo.pt\n",
    "    for site in sites:\n",
    "        print(f\"Extraindo dados de {site}\")\n",
    "        for model in models:\n",
    "            for periodo in periodos:\n",
    "                print(f\"Extraindo dados para {model} de {periodo[0]} a {periodo[1]}\")\n",
    "                data = arquivo_search(query=model, site=site, from_year=periodo[0], to_year=periodo[1])\n",
    "                \n",
    "                # Verificar se a resposta contém a chave 'response_items' e se não está vazia\n",
    "                if data and 'response_items' in data and data['response_items']:\n",
    "                    # Transformar os dados em um DataFrame do pandas\n",
    "\n",
    "                    df = pd.DataFrame(data['response_items'])\n",
    "                    \n",
    "                    # Verificar se a coluna 'title' existe no DataFrame\n",
    "                    if 'title' in df.columns:\n",
    "                        df['site'] = site\n",
    "                        df['model'] = model\n",
    "                        df['periodo'] = f\"{periodo[0]}-{periodo[1]}\"\n",
    "                        cols = ['site', 'model', 'periodo'] + [col for col in df.columns if col not in ['site', 'model', 'periodo']]\n",
    "                        df = df[cols]\n",
    "                        # Filtrar os resultados indesejados (capas, vidros, etc.)\n",
    "                        df = df[~df['title'].str.contains('capa|vidro|película|selfie', case=False, na=False)]\n",
    "                        # Filtrar para garantir que o nome do modelo esteja no título\n",
    "                        df = df[df['title'].str.contains(model, case=False, na=False)]\n",
    "                        \n",
    "                        \n",
    "                        # Adicionar os dados à lista de informações\n",
    "                        info.append(df)\n",
    "                    else:\n",
    "                        print(f\"A coluna 'title' não foi encontrada nos dados retornados para '{model}' no site '{site}' de {periodo[0]} a {periodo[1]}.\")\n",
    "                else:\n",
    "                    print(f\"Nenhum resultado encontrado para '{model}' no site '{site}' de {periodo[0]} a {periodo[1]}.\")\n",
    "\n",
    "    # Concatenar todos os DataFrames em um único DataFrame\n",
    "    if info:\n",
    "        result_df = pd.concat(info, ignore_index=True)\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"Nenhum dado relevante foi encontrado.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo dados de www.fnac.pt\n",
      "Extraindo dados para huawei p8 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'huawei p8' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para huawei p8 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'huawei p8' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para huawei p8 de 2016 a 2020\n",
      "Extraindo dados para huawei p8 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'huawei p8' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para huawei p20 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'huawei p20' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para huawei p20 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'huawei p20' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para huawei p20 de 2016 a 2020\n",
      "Extraindo dados para huawei p20 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'huawei p20' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para huawei p30 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'huawei p30' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para huawei p30 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'huawei p30' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para huawei p30 de 2016 a 2020\n",
      "Extraindo dados para huawei p30 de 2021 a 2024\n",
      "Extraindo dados para huawei p40 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'huawei p40' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para huawei p40 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'huawei p40' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para huawei p40 de 2016 a 2020\n",
      "Nenhum resultado encontrado para 'huawei p40' no site 'www.fnac.pt' de 2016 a 2020.\n",
      "Extraindo dados para huawei p40 de 2021 a 2024\n",
      "Extraindo dados para huawei p50 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'huawei p50' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para huawei p50 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'huawei p50' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para huawei p50 de 2016 a 2020\n",
      "Extraindo dados para huawei p50 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'huawei p50' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para samsung galaxy s8 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s8' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s8 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s8' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s8 de 2016 a 2020\n",
      "Extraindo dados para samsung galaxy s8 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'samsung galaxy s8' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para samsung galaxy s9 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s9' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s9 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s9' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s9 de 2016 a 2020\n",
      "Extraindo dados para samsung galaxy s9 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'samsung galaxy s9' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para samsung galaxy s10 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s10' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s10 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s10' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s10 de 2016 a 2020\n",
      "Extraindo dados para samsung galaxy s10 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'samsung galaxy s10' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para samsung galaxy s20 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s20' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s20 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s20' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s20 de 2016 a 2020\n",
      "Nenhum resultado encontrado para 'samsung galaxy s20' no site 'www.fnac.pt' de 2016 a 2020.\n",
      "Extraindo dados para samsung galaxy s20 de 2021 a 2024\n",
      "Extraindo dados para samsung galaxy s21 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s21' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s21 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s21' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s21 de 2016 a 2020\n",
      "Nenhum resultado encontrado para 'samsung galaxy s21' no site 'www.fnac.pt' de 2016 a 2020.\n",
      "Extraindo dados para samsung galaxy s21 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'samsung galaxy s21' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para samsung galaxy s22 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'samsung galaxy s22' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para samsung galaxy s22 de 2010 a 2015\n",
      "Nenhum resultado encontrado para 'samsung galaxy s22' no site 'www.fnac.pt' de 2010 a 2015.\n",
      "Extraindo dados para samsung galaxy s22 de 2016 a 2020\n",
      "Nenhum resultado encontrado para 'samsung galaxy s22' no site 'www.fnac.pt' de 2016 a 2020.\n",
      "Extraindo dados para samsung galaxy s22 de 2021 a 2024\n",
      "Nenhum resultado encontrado para 'samsung galaxy s22' no site 'www.fnac.pt' de 2021 a 2024.\n",
      "Extraindo dados para apple iphone 6 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'apple iphone 6' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para apple iphone 6 de 2010 a 2015\n",
      "Extraindo dados para apple iphone 6 de 2016 a 2020\n",
      "Extraindo dados para apple iphone 6 de 2021 a 2024\n",
      "Extraindo dados para apple iphone 7 de 2005 a 2009\n",
      "Nenhum resultado encontrado para 'apple iphone 7' no site 'www.fnac.pt' de 2005 a 2009.\n",
      "Extraindo dados para apple iphone 7 de 2010 a 2015\n",
      "Extraindo dados para apple iphone 7 de 2016 a 2020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei p8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei p20\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei p30\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei p40\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei p50\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s9\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s20\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s21\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamsung galaxy s22\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone xs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone xr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 11\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 12\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 13\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple iphone 14\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei mate 20\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei mate 30\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuawei mate 40\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Coletar dados dos modelos nos sites, divididos por período\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m resultados \u001b[38;5;241m=\u001b[39m coletar_dados_por_periodo(models, sites, periodos)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Exibir os resultados\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resultados \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# transforma os resultadosnum csv\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m, in \u001b[0;36mcoletar_dados_por_periodo\u001b[0;34m(models, sites, periodos)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m periodo \u001b[38;5;129;01min\u001b[39;00m periodos:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo dados para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiodo[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiodo[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     data \u001b[38;5;241m=\u001b[39m arquivo_search(query\u001b[38;5;241m=\u001b[39mmodel, site\u001b[38;5;241m=\u001b[39msite, from_year\u001b[38;5;241m=\u001b[39mperiodo[\u001b[38;5;241m0\u001b[39m], to_year\u001b[38;5;241m=\u001b[39mperiodo[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Verificar se a resposta contém a chave 'response_items' e se não está vazia\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_items\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_items\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Transformar os dados em um DataFrame do pandas\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36marquivo_search\u001b[0;34m(query, max_items, from_year, to_year, site, doc_type, version_history_url)\u001b[0m\n\u001b[1;32m     16\u001b[0m     base_url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m base_url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&maxItems=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&prettyPrint=false\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "periodos = [(2005,2009),(2010, 2015), (2016, 2020), (2021, 2024)]\n",
    "\n",
    "models = [\n",
    "    'huawei p8', 'huawei p20', 'huawei p30', 'huawei p40', 'huawei p50',\n",
    "    'samsung galaxy s8', 'samsung galaxy s9', 'samsung galaxy s10', 'samsung galaxy s20', 'samsung galaxy s21', 'samsung galaxy s22',\n",
    "    'apple iphone 6', 'apple iphone 7', 'apple iphone 8', 'apple iphone x', 'apple iphone xs', 'apple iphone xr', 'apple iphone 11', 'apple iphone 12', 'apple iphone 13', 'apple iphone 14',\n",
    "    'huawei mate 20', 'huawei mate 30', 'huawei mate 40'\n",
    "]\n",
    "\n",
    "\n",
    "# Coletar dados dos modelos nos sites, divididos por período\n",
    "resultados = coletar_dados_por_periodo(models, sites, periodos)\n",
    "\n",
    "# Exibir os resultados\n",
    "if resultados is not None:\n",
    "    # transforma os resultadosnum csv\n",
    "    resultados.to_csv('data/resultados.csv', index=False)\n",
    "else:\n",
    "    print(\"Nenhum resultado foi extraído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
