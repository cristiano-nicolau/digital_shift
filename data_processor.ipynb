{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import ast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta secção, vamos juntar todos os produtos extraidos  de cada um dos sites seja em black friday ou não, vamos criar um dataset com todos os produtos e respectivas informações. Depois vamos fazer uma limpeza dos dados repetidos e por fim aplicar um modelo de agregação e categorização dos produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"data/*products_extracted_prices*.csv\") # with glob we can get all the files that match the pattern\n",
    "dataframes = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if \"Black_Friday\" in file_path:\n",
    "        df['promocao'] = 'Black Friday'\n",
    "    else:\n",
    "        df['promocao'] = 'Sem Promocao'\n",
    "    \n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(\"all_prices_extracted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vou so fazer um teste para ter a certeza que o codigo acima funcionou como pretendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Black Friday rows across all files: 839\n",
      "Total Black Friday rows on merged DataFrame: 839\n",
      "Total Not Black Friday rows across all files: 703\n",
      "Total Not Black Friday rows on merged DataFrame: 703\n"
     ]
    }
   ],
   "source": [
    "file_paths = glob.glob(\"data/*products_extracted_prices*.csv\") \n",
    "\n",
    "black_friday_counts = {}\n",
    "not_black_friday_counts = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if \"Black_Friday\" in file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "        black_friday_counts[file_path] = df.shape[0]\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "        not_black_friday_counts[file_path] = df.shape[0]\n",
    "        \n",
    "total_black_friday_rows = sum(black_friday_counts.values())\n",
    "print(f\"Total Black Friday rows across all files: {total_black_friday_rows}\")\n",
    "total_black_friday_rows_on_merged_df = merged_df[merged_df['promocao'] == 'Black Friday'].shape[0]\n",
    "print(f\"Total Black Friday rows on merged DataFrame: {total_black_friday_rows_on_merged_df}\")\n",
    "\n",
    "total_not_black_friday_rows = sum(not_black_friday_counts.values())\n",
    "print(f\"Total Not Black Friday rows across all files: {total_not_black_friday_rows}\")\n",
    "total_not_black_friday_rows_on_merged_df = merged_df[merged_df['promocao'] == 'Sem Promocao'].shape[0]\n",
    "print(f\"Total Not Black Friday rows on merged DataFrame: {total_not_black_friday_rows_on_merged_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"all_prices_extracted.csv\")\n",
    "merged_df = merged_df.drop(columns=['linkToArchive','linkToExtractedText','snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site              0\n",
       "date              0\n",
       "title            16\n",
       "extractedData    55\n",
       "promocao          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site             0\n",
       "date             0\n",
       "title            0\n",
       "extractedData    0\n",
       "promocao         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (1476, 5)\n",
      "Shape after: (967, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape before: {merged_df.shape}\")\n",
    "\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "print(f\"Shape after: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_data(extracted_str):\n",
    "    try:\n",
    "        extracted_list = ast.literal_eval(extracted_str) # é suado para converter as strings em listas\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  \n",
    "\n",
    "    cleaned_prices = []\n",
    "    for price in extracted_list:\n",
    "        normalized_price = unicodedata.normalize(\"NFKD\", price)\n",
    "        cleaned_price = re.sub(r'[^\\d,]', '', normalized_price)\n",
    "        cleaned_prices.append(cleaned_price)\n",
    "    \n",
    "    return cleaned_prices\n",
    "\n",
    "merged_df['extractedData'] = merged_df['extractedData'].apply(clean_extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"Dual\", \"SIM\", \"Sim\", \"Staples.pt\", \"|\", \"Â\", \"Tecnologia\", \"El\" ,\"Corte\" \"Â·\", \"Android\", '\"\"', '\"', \",\", \"InglÃ©s\", \"InglÃ©s\", \"Inglés\", \"·\",  \"Sabe\", \"mais\",  \"em\", \"Fnac.pt\", \"Compra\",\"Comprar\", \"na\", \"(\", \"Product\", \")\", \"Worten.pt\", \"Worten\", \"pt\", \"WORTEN\", \"-\", '\"', \"Wi-Fi\", \"Cuidados\",  \"Ecológicos\",\"Vida\", \"Sustentável\", \"Radio\", \"Popular\", \"PCDIGA\", \"(Product)\"]\n",
    "\n",
    "def clean_title(title):\n",
    "    title = title.split()\n",
    "    title = [word for word in title if word not in stop_words]\n",
    "    title = ' '.join(title)\n",
    "    return title\n",
    "\n",
    "merged_df['title'] = merged_df['title'].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quero retirar cenas do genero N.N\"\" ou N.N'' onde NN é um número, quero que substitua por \n",
    "def clean_title(title):\n",
    "    title = re.sub(r'\\d+\\.\\d+[\"\\']', '', title)\n",
    "    return title\n",
    "\n",
    "merged_df['title'] = merged_df['title'].apply(clean_title)\n",
    "\n",
    "# quero retirar ' \n",
    "def clean_title(title):\n",
    "    title = re.sub(r\"'\", '', title)\n",
    "    return title\n",
    "\n",
    "merged_df['title'] = merged_df['title'].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"all_prices_extracted_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site             0\n",
       "date             0\n",
       "title            3\n",
       "extractedData    0\n",
       "promocao         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_prices_extracted_cleaned.csv\")\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (964, 5)\n",
      "Duplicatas encontradas:  70\n",
      "Shape after: (925, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape before: {df.shape}\")\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "        Vamos remover todos os duplicados que sejam iguais nas colunas 'site', 'title', 'date' e 'promocao'\n",
    "    '''\n",
    "    df = df.sort_values(by=['site', 'date', 'title', 'promocao'])\n",
    "\n",
    "    duplicates = df[df.duplicated(subset=['site', 'date', 'title', 'promocao'], keep=False)]\n",
    "    print(\"Duplicatas encontradas: \", duplicates.shape[0])\n",
    "\n",
    "\n",
    "    df = df.drop_duplicates(subset=['site', 'title', 'date', 'promocao'], keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Chama a função para remover as duplicatas\n",
    "df = remove_duplicates(df)\n",
    "\n",
    "print(f\"Shape after: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "df['extractedData'] = df['extractedData'].apply(ast.literal_eval)\n",
    "\n",
    "def convert_to_float_list(data):\n",
    "    return [float(x.replace(',', '.')) for x in data]\n",
    "\n",
    "df['extractedData'] = df['extractedData'].apply(convert_to_float_list)\n",
    "\n",
    "# quero ver o tipo da coluna extractedData, e o tipo do elemento da lista\n",
    "print(df['extractedData'].dtype)\n",
    "print(type(df['extractedData'].iloc[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_www(data):\n",
    "    return data.split('.')[1]\n",
    "\n",
    "df['site'] = df['site'].apply(cleaned_www)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_prices_extracted_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
