{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Shift: The Evolution of Products and Platforms in Portuguese E-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Processor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"www.fnac.pt\", \"www.worten.pt\", \"www.elcorteingles.pt\", \"www.radiopopular.pt\", \"www.staples.pt\", \"www.pcdiga.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Extracting product categories from the websites (2007 | 2010 | 2015 | 2020 | 2023)\n",
    "\n",
    "We will use the BeautifulSoup library to extract the product categories from the websites, and we will use the requests library to get the HTML content of the websites.\n",
    "We had to create a different script for each year and site, because the structure of the sites is different for each year and site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>www.fnac.pt</th>\n",
       "      <th>www.worten.pt</th>\n",
       "      <th>www.elcorteingles.pt</th>\n",
       "      <th>www.radiopopular.pt</th>\n",
       "      <th>www.staples.pt</th>\n",
       "      <th>www.pcdiga.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arquivo.pt/wayback/20050725031922/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050722223614/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050722174753/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050719124815/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>https://arquivo.pt/wayback/20061118120805/http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arquivo.pt/wayback/20060216170739/http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>https://arquivo.pt/wayback/20070928223117/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070611190104/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070929080902/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070929122436/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070610185333/http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>https://arquivo.pt/wayback/20081027081756/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022044251/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081021184312/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022013802/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022031557/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022130111/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218064527/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218174523/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218054927/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218134419/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218154357/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091219171727/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        www.fnac.pt  \\\n",
       "0        2005                                                NaN   \n",
       "1        2006  https://arquivo.pt/wayback/20061118120805/http...   \n",
       "2        2007  https://arquivo.pt/wayback/20070928223117/http...   \n",
       "3        2008  https://arquivo.pt/wayback/20081027081756/http...   \n",
       "4        2009  https://arquivo.pt/wayback/20091218064527/http...   \n",
       "\n",
       "                                       www.worten.pt  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070611190104/http...   \n",
       "3  https://arquivo.pt/wayback/20081022044251/http...   \n",
       "4  https://arquivo.pt/wayback/20091218174523/http...   \n",
       "\n",
       "                                www.elcorteingles.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050725031922/http...   \n",
       "1  https://arquivo.pt/wayback/20060216170739/http...   \n",
       "2  https://arquivo.pt/wayback/20070929080902/http...   \n",
       "3  https://arquivo.pt/wayback/20081021184312/http...   \n",
       "4  https://arquivo.pt/wayback/20091218054927/http...   \n",
       "\n",
       "                                 www.radiopopular.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050722223614/http...   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070929122436/http...   \n",
       "3  https://arquivo.pt/wayback/20081022013802/http...   \n",
       "4  https://arquivo.pt/wayback/20091218134419/http...   \n",
       "\n",
       "                                      www.staples.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050722174753/http...   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070610185333/http...   \n",
       "3  https://arquivo.pt/wayback/20081022031557/http...   \n",
       "4  https://arquivo.pt/wayback/20091218154357/http...   \n",
       "\n",
       "                                      www.pcdiga.com  \n",
       "0  https://arquivo.pt/wayback/20050719124815/http...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  https://arquivo.pt/wayback/20081022130111/http...  \n",
       "4  https://arquivo.pt/wayback/20091219171727/http...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/sites_links.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Unnamed: 0            19 non-null     int64 \n",
      " 1   www.fnac.pt           16 non-null     object\n",
      " 2   www.worten.pt         14 non-null     object\n",
      " 3   www.elcorteingles.pt  16 non-null     object\n",
      " 4   www.radiopopular.pt   17 non-null     object\n",
      " 5   www.staples.pt        18 non-null     object\n",
      " 6   www.pcdiga.com        16 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0              0\n",
      "www.fnac.pt             3\n",
      "www.worten.pt           5\n",
      "www.elcorteingles.pt    3\n",
      "www.radiopopular.pt     2\n",
      "www.staples.pt          1\n",
      "www.pcdiga.com          3\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>www.fnac.pt</th>\n",
       "      <th>www.worten.pt</th>\n",
       "      <th>www.elcorteingles.pt</th>\n",
       "      <th>www.radiopopular.pt</th>\n",
       "      <th>www.staples.pt</th>\n",
       "      <th>www.pcdiga.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  www.fnac.pt  www.worten.pt  www.elcorteingles.pt  \\\n",
       "0        False         True           True                 False   \n",
       "1        False        False           True                 False   \n",
       "2        False        False          False                 False   \n",
       "3        False        False          False                 False   \n",
       "4        False        False          False                 False   \n",
       "5        False        False          False                 False   \n",
       "6        False        False          False                 False   \n",
       "7        False        False          False                 False   \n",
       "8        False        False          False                 False   \n",
       "9        False        False          False                 False   \n",
       "10       False        False          False                 False   \n",
       "11       False        False          False                 False   \n",
       "12       False         True           True                 False   \n",
       "13       False        False          False                  True   \n",
       "14       False         True           True                  True   \n",
       "15       False        False          False                 False   \n",
       "16       False        False           True                  True   \n",
       "17       False        False          False                 False   \n",
       "18       False        False          False                 False   \n",
       "\n",
       "    www.radiopopular.pt  www.staples.pt  www.pcdiga.com  \n",
       "0                 False           False           False  \n",
       "1                  True            True            True  \n",
       "2                 False           False            True  \n",
       "3                 False           False           False  \n",
       "4                 False           False           False  \n",
       "5                 False           False           False  \n",
       "6                 False           False           False  \n",
       "7                 False           False            True  \n",
       "8                 False           False           False  \n",
       "9                 False           False           False  \n",
       "10                False           False           False  \n",
       "11                False           False           False  \n",
       "12                False           False           False  \n",
       "13                False           False           False  \n",
       "14                False           False           False  \n",
       "15                False           False           False  \n",
       "16                 True           False           False  \n",
       "17                False           False           False  \n",
       "18                False           False           False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'www.fnac.pt', 'www.worten.pt', 'www.elcorteingles.pt',\n",
       "       'www.radiopopular.pt', 'www.staples.pt', 'www.pcdiga.com'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['ano', 'link', 'site', 'numero_categorias', 'lista_categorias', 'dicionario_subcategorias']\n",
    "header_df = pd.DataFrame(columns=header)\n",
    "header_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_2007_worten(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('ul', class_='menu')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_name = category.find('a').get_text(strip=True)\n",
    "            \n",
    "        # nao tem subcategorias\n",
    "            categories_dict[main_category_name] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        return 0, {}\n",
    "    \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year <= 2012:\n",
    "                link = link + 'Splash.aspx'\n",
    "            if year == 2007:\n",
    "                num_categories, category_dict = extractor_2007_worten(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='menu menu-category')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='sub'):\n",
    "                main_category_name = category.find('a', class_='label').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('a', class_='label').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            link = link + 'default.aspx'    \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', id='nav')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='level1'):\n",
    "                main_category_name = category.find('a').find('span').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='level2'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            # se houver subcategorias com [] é porque não tem subcategorias, e eliminar categoria\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values or site_column not in category_analysis['site'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_worten_2020_23(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='nav-sub js-nav-sub')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='nav-item nav-item-sub'):\n",
    "                \n",
    "                main_category_name = category.find('span', class_='nav-a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul', class_='nav nav-sub nav-sub-child js-nav-sub')\n",
    "                for sub in subcategories_container.find_all('li', class_='nav-item-sub'):\n",
    "                    label = sub.find('label', class_='nav-trigger js-nav-trigger')\n",
    "                    if label:\n",
    "                        subcategory_name = label.find('a', class_='nav-a')\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name.get_text(strip=True).lower())\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_worten_2020_23(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staples Extracted Categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_2007_10(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('span', id= lambda x: x and 'categorias' in x.lower())\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='tracos_centro'):\n",
    "            a_tag = div.find('a')\n",
    "            if not a_tag:\n",
    "                continue\n",
    "\n",
    "            category_name = a_tag.get_text(strip=True)\n",
    "\n",
    "            if 'tit_centro_blue_bold' in a_tag.get('class', []):\n",
    "                # Main category\n",
    "                current_main_category = category_name\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                # Subcategory\n",
    "                if current_main_category:\n",
    "                    categories_dict[current_main_category].append(\n",
    "                         category_name\n",
    "                    )\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'default.aspx'\n",
    "            if year == 2007 or year == 2010:\n",
    "                num_categories, category_dict = get_categories_staples_2007_10(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_staples_15(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_staples_20(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_23(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='container-menu-children')\n",
    "        print(categories_container)\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='children-cont'):\n",
    "            main_category_tag = div.find('span', class_='pr-name')\n",
    "            if main_category_tag:\n",
    "                current_main_category = main_category_tag.get_text(strip=True)\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                if current_main_category:\n",
    "                    subcategory_link = div.find('a', class_='menu-link_a')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.get_text(strip=True)\n",
    "                        categories_dict[current_main_category].append(subcategory_name)\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2023:\n",
    "                num_categories, category_dict = get_categories_staples_23(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PcDiga Extracted Categories:\n",
    "- As it is not possible to extract categories from 2007, we will extract from 2008, assuming that the differences between 2007 and 2008 are minimal.\n",
    "- As it is not possible to extract categories from 2023, we assume that the differences between 2020 and 2023 are minimal, or probably the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_08(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('map', attrs={'name': 'Map'})\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('area'):\n",
    "            # extrair o nome do href = ...?Familia=nome\n",
    "            category_href = category\n",
    "            main_category_name = category.get('href').split('=')[-1]\n",
    "\n",
    "            subcategories = []\n",
    "\n",
    "            # a categoria principal não tem subcategorias\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'pcdiga/'\n",
    "            if year == 2008 or year == 2010:\n",
    "                n_year = year - 1 if year == 2008 else year\n",
    "                num_categories, category_dict = get_categories_pcdiga_08(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': n_year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('div', id='masterdiv')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('table', class_='menu1'):\n",
    "            main_category_name = category.find('a', class_='menu').get_text(strip=True)\n",
    "            subcategories = []\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "\n",
    "                num_categories, category_dict = get_categories_pcdiga_15(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories = soup.find('div', class_='megamenu-wrapper')\n",
    "        categories_container = categories.find('ul', class_='megamenu')\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_link = category.find('a', class_='i-link')\n",
    "            svg_element = category.find('span', class_='svg-i svg-arrow')\n",
    "            if not main_category_link or not svg_element:\n",
    "                continue  \n",
    "\n",
    "            main_category_name = main_category_link.find('span', class_='lnk-text').get_text(strip=True)\n",
    "\n",
    "            subcategories = []\n",
    "            subcategories_container = category.find('div', class_='submenu')\n",
    "            if subcategories_container:\n",
    "                for subcategory in subcategories_container.find_all('li'):\n",
    "                    subcategory_link = subcategory.find('a', class_='i-link')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)  \n",
    "\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_pcdiga_20(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RadioPopular Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in soup.find_all('td', align='left'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.find('img').get('alt')\n",
    "                    categories_dict[main_category_name] = []\n",
    "            for i in list(categories_dict.keys()):\n",
    "                if i == 'Recrutamento':\n",
    "                    del categories_dict[i]\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010 or year == 2007:\n",
    "                num_categories, category_dict = get_categories_2010_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            div = soup.find('div', id='nav')\n",
    "            if div is None:\n",
    "                return 0, {} \n",
    "\n",
    "            for category in div.find_all('li', class_='dir'):\n",
    "                main_category_name = category.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='dir'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                if subcategories:\n",
    "                    categories_dict[main_category_name] = subcategories\n",
    "                else:\n",
    "                    categories_dict[main_category_name] = []\n",
    "\n",
    "            # vamos ver todas as keys que tem [] e eliminar, pois não tem subcategorias\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            ul = soup.find('ul', class_='categories')\n",
    "\n",
    "            for i in ul.find_all('li', class_='category link cb'):\n",
    "                main_category_name = i.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "                div = i.find('div', class_='subcategories')\n",
    "                if div:\n",
    "                    for subcategory in div.find_all('li', class_=\"subcategory family link\"):\n",
    "                        subcategory_name = subcategory.get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')    \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_2020_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As elcorreioingles does not have product categories, we will not use this site to make this comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fnac Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is not possible to extract categories, from 2006 and 2008, we will not use the year 2007 in fnac to make the comparison\n",
    "\n",
    "Error message presented:```O seu browser não aceita cookies, pelo que não é possível o acesso ao nosso site.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='menu')\n",
    "            u = div.find('ul', style=\"margin-left: 40px\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                ul = category.find('ul')\n",
    "                if ul:\n",
    "                    for subcategory in ul.find_all('li'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategory_name = subcategory_name.replace('»', '').replace('\\r\\n', '').strip()\n",
    "                        subcategory_name = ' '.join(subcategory_name.split())\n",
    "                        subcategories.append(subcategory_name)\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "            for value in categories_dict.values():\n",
    "                if 'Ver todos os produtos' in value:\n",
    "                    value.remove('Ver todos os produtos')\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='MENU')\n",
    "            u = div.find('ul', id=\"onglets\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    span = a.find('span', class_='inner')\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                \n",
    "                div = category.find('div', class_='megaMenu')\n",
    "                if div:\n",
    "                    for sub in div.find_all('dt'):\n",
    "                        # pode ja ter subcategoria ou ter um <a> com a subcategoria\n",
    "                        subcategory_name = sub.get_text(strip=True)\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name)\n",
    "\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='Sidebar-nano nano')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"CategoryNav js-CategoryNav\")\n",
    "            for il in ul.find_all('li', class_='CategoryNav-item js-CategoryNav-item'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_2020_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2023_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='SideNavPanel-listWrapper')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"SideNavPanel-list\")\n",
    "            for il in ul.find_all('li', class_='SideNavPanel-listItem js-SideNavPanel-listItem'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2023:\n",
    "\n",
    "                num_categories, category_dict = get_categories_2023_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
