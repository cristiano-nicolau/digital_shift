{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Shift: The Evolution of Products and Platforms in Portuguese E-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Processor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"www.fnac.pt\", \"www.worten.pt\", \"www.elcorteingles.pt\", \"www.radiopopular.pt\", \"www.staples.pt\", \"www.pcdiga.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Extracting product categories from the websites (2007 | 2010 | 2015 | 2020 | 2023)\n",
    "\n",
    "We will use the BeautifulSoup library to extract the product categories from the websites, and we will use the requests library to get the HTML content of the websites.\n",
    "We had to create a different script for each year and site, because the structure of the sites is different for each year and site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>www.fnac.pt</th>\n",
       "      <th>www.worten.pt</th>\n",
       "      <th>www.elcorteingles.pt</th>\n",
       "      <th>www.radiopopular.pt</th>\n",
       "      <th>www.staples.pt</th>\n",
       "      <th>www.pcdiga.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arquivo.pt/wayback/20050725031922/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050722223614/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050722174753/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20050719124815/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>https://arquivo.pt/wayback/20061118120805/http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arquivo.pt/wayback/20060216170739/http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>https://arquivo.pt/wayback/20070928223117/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070611190104/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070929080902/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070929122436/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20070610185333/http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>https://arquivo.pt/wayback/20081027081756/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022044251/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081021184312/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022013802/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022031557/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20081022130111/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218064527/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218174523/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218054927/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218134419/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091218154357/http...</td>\n",
       "      <td>https://arquivo.pt/wayback/20091219171727/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        www.fnac.pt  \\\n",
       "0        2005                                                NaN   \n",
       "1        2006  https://arquivo.pt/wayback/20061118120805/http...   \n",
       "2        2007  https://arquivo.pt/wayback/20070928223117/http...   \n",
       "3        2008  https://arquivo.pt/wayback/20081027081756/http...   \n",
       "4        2009  https://arquivo.pt/wayback/20091218064527/http...   \n",
       "\n",
       "                                       www.worten.pt  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070611190104/http...   \n",
       "3  https://arquivo.pt/wayback/20081022044251/http...   \n",
       "4  https://arquivo.pt/wayback/20091218174523/http...   \n",
       "\n",
       "                                www.elcorteingles.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050725031922/http...   \n",
       "1  https://arquivo.pt/wayback/20060216170739/http...   \n",
       "2  https://arquivo.pt/wayback/20070929080902/http...   \n",
       "3  https://arquivo.pt/wayback/20081021184312/http...   \n",
       "4  https://arquivo.pt/wayback/20091218054927/http...   \n",
       "\n",
       "                                 www.radiopopular.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050722223614/http...   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070929122436/http...   \n",
       "3  https://arquivo.pt/wayback/20081022013802/http...   \n",
       "4  https://arquivo.pt/wayback/20091218134419/http...   \n",
       "\n",
       "                                      www.staples.pt  \\\n",
       "0  https://arquivo.pt/wayback/20050722174753/http...   \n",
       "1                                                NaN   \n",
       "2  https://arquivo.pt/wayback/20070610185333/http...   \n",
       "3  https://arquivo.pt/wayback/20081022031557/http...   \n",
       "4  https://arquivo.pt/wayback/20091218154357/http...   \n",
       "\n",
       "                                      www.pcdiga.com  \n",
       "0  https://arquivo.pt/wayback/20050719124815/http...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  https://arquivo.pt/wayback/20081022130111/http...  \n",
       "4  https://arquivo.pt/wayback/20091219171727/http...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/sites_links.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Unnamed: 0            19 non-null     int64 \n",
      " 1   www.fnac.pt           16 non-null     object\n",
      " 2   www.worten.pt         14 non-null     object\n",
      " 3   www.elcorteingles.pt  16 non-null     object\n",
      " 4   www.radiopopular.pt   17 non-null     object\n",
      " 5   www.staples.pt        18 non-null     object\n",
      " 6   www.pcdiga.com        16 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0              0\n",
      "www.fnac.pt             3\n",
      "www.worten.pt           5\n",
      "www.elcorteingles.pt    3\n",
      "www.radiopopular.pt     2\n",
      "www.staples.pt          1\n",
      "www.pcdiga.com          3\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>www.fnac.pt</th>\n",
       "      <th>www.worten.pt</th>\n",
       "      <th>www.elcorteingles.pt</th>\n",
       "      <th>www.radiopopular.pt</th>\n",
       "      <th>www.staples.pt</th>\n",
       "      <th>www.pcdiga.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  www.fnac.pt  www.worten.pt  www.elcorteingles.pt  \\\n",
       "0        False         True           True                 False   \n",
       "1        False        False           True                 False   \n",
       "2        False        False          False                 False   \n",
       "3        False        False          False                 False   \n",
       "4        False        False          False                 False   \n",
       "5        False        False          False                 False   \n",
       "6        False        False          False                 False   \n",
       "7        False        False          False                 False   \n",
       "8        False        False          False                 False   \n",
       "9        False        False          False                 False   \n",
       "10       False        False          False                 False   \n",
       "11       False        False          False                 False   \n",
       "12       False         True           True                 False   \n",
       "13       False        False          False                  True   \n",
       "14       False         True           True                  True   \n",
       "15       False        False          False                 False   \n",
       "16       False        False           True                  True   \n",
       "17       False        False          False                 False   \n",
       "18       False        False          False                 False   \n",
       "\n",
       "    www.radiopopular.pt  www.staples.pt  www.pcdiga.com  \n",
       "0                 False           False           False  \n",
       "1                  True            True            True  \n",
       "2                 False           False            True  \n",
       "3                 False           False           False  \n",
       "4                 False           False           False  \n",
       "5                 False           False           False  \n",
       "6                 False           False           False  \n",
       "7                 False           False            True  \n",
       "8                 False           False           False  \n",
       "9                 False           False           False  \n",
       "10                False           False           False  \n",
       "11                False           False           False  \n",
       "12                False           False           False  \n",
       "13                False           False           False  \n",
       "14                False           False           False  \n",
       "15                False           False           False  \n",
       "16                 True           False           False  \n",
       "17                False           False           False  \n",
       "18                False           False           False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'www.fnac.pt', 'www.worten.pt', 'www.elcorteingles.pt',\n",
       "       'www.radiopopular.pt', 'www.staples.pt', 'www.pcdiga.com'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['ano', 'link', 'site', 'numero_categorias', 'lista_categorias', 'dicionario_subcategorias']\n",
    "header_df = pd.DataFrame(columns=header)\n",
    "header_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_2007_worten(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('ul', class_='menu')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_name = category.find('a').get_text(strip=True)\n",
    "            \n",
    "        # nao tem subcategorias\n",
    "            categories_dict[main_category_name] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        return 0, {}\n",
    "    \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year <= 2012:\n",
    "                link = link + 'Splash.aspx'\n",
    "            if year == 2007:\n",
    "                num_categories, category_dict = extractor_2007_worten(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='menu menu-category')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='sub'):\n",
    "                main_category_name = category.find('a', class_='label').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('a', class_='label').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            link = link + 'default.aspx'    \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_worten(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', id='nav')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='level1'):\n",
    "                main_category_name = category.find('a').find('span').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='level2'):\n",
    "                        # Extract subcategory name\n",
    "                        subcategory_name = subcategory.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            # se houver subcategorias com [] é porque não tem subcategorias, e eliminar categoria\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values or site_column not in category_analysis['site'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_worten(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_worten_2020_23(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "            categories_container = soup.find('ul', class_='nav-sub js-nav-sub')\n",
    "\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in categories_container.find_all('li', class_='nav-item nav-item-sub'):\n",
    "                \n",
    "                main_category_name = category.find('span', class_='nav-a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul', class_='nav nav-sub nav-sub-child js-nav-sub')\n",
    "                for sub in subcategories_container.find_all('li', class_='nav-item-sub'):\n",
    "                    label = sub.find('label', class_='nav-trigger js-nav-trigger')\n",
    "                    if label:\n",
    "                        subcategory_name = label.find('a', class_='nav-a')\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name.get_text(strip=True).lower())\n",
    "\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.worten.pt' and (year not in category_analysis['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_worten_2020_23(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staples Extracted Categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_2007_10(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('span', id= lambda x: x and 'categorias' in x.lower())\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='tracos_centro'):\n",
    "            a_tag = div.find('a')\n",
    "            if not a_tag:\n",
    "                continue\n",
    "\n",
    "            category_name = a_tag.get_text(strip=True)\n",
    "\n",
    "            if 'tit_centro_blue_bold' in a_tag.get('class', []):\n",
    "                # Main category\n",
    "                current_main_category = category_name\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                # Subcategory\n",
    "                if current_main_category:\n",
    "                    categories_dict[current_main_category].append(\n",
    "                         category_name\n",
    "                    )\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'default.aspx'\n",
    "            if year == 2007 or year == 2010:\n",
    "                num_categories, category_dict = get_categories_staples_2007_10(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_staples_15(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='primaryNav')\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('span', class_='navItem'):\n",
    "            current_main_category = div.find('a', class_='navLink').get_text(strip=True)\n",
    "            categories_dict[current_main_category] = []\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_staples_20(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_staples_23(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        # span com id que contem a palavra categorias\n",
    "        categories_container = soup.find('div', class_='container-menu-children')\n",
    "        print(categories_container)\n",
    "        categories_dict = {}\n",
    "\n",
    "        current_main_category = None\n",
    "        for div in categories_container.find_all('div', class_='children-cont'):\n",
    "            main_category_tag = div.find('span', class_='pr-name')\n",
    "            if main_category_tag:\n",
    "                current_main_category = main_category_tag.get_text(strip=True)\n",
    "                categories_dict[current_main_category] = []\n",
    "            else:\n",
    "                if current_main_category:\n",
    "                    subcategory_link = div.find('a', class_='menu-link_a')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.get_text(strip=True)\n",
    "                        categories_dict[current_main_category].append(subcategory_name)\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.staples.pt' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2023:\n",
    "                num_categories, category_dict = get_categories_staples_23(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PcDiga Extracted Categories:\n",
    "- As it is not possible to extract categories from 2007, we will extract from 2008, assuming that the differences between 2007 and 2008 are minimal.\n",
    "- As it is not possible to extract categories from 2023, we assume that the differences between 2020 and 2023 are minimal, or probably the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_08(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('map', attrs={'name': 'Map'})\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('area'):\n",
    "            # extrair o nome do href = ...?Familia=nome\n",
    "            category_href = category\n",
    "            main_category_name = category.get('href').split('=')[-1]\n",
    "\n",
    "            subcategories = []\n",
    "\n",
    "            # a categoria principal não tem subcategorias\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            link += 'pcdiga/'\n",
    "            if year == 2008 or year == 2010:\n",
    "                n_year = year - 1 if year == 2008 else year\n",
    "                num_categories, category_dict = get_categories_pcdiga_08(link)\n",
    "\n",
    "                category_data.append({\n",
    "                    'ano': n_year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_15(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories_container = soup.find('div', id='masterdiv')\n",
    "\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('table', class_='menu1'):\n",
    "            main_category_name = category.find('a', class_='menu').get_text(strip=True)\n",
    "            subcategories = []\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2015:\n",
    "\n",
    "                num_categories, category_dict = get_categories_pcdiga_15(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_pcdiga_20(url):\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        request.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        categories = soup.find('div', class_='megamenu-wrapper')\n",
    "        categories_container = categories.find('ul', class_='megamenu')\n",
    "        categories_dict = {}\n",
    "\n",
    "        for category in categories_container.find_all('li'):\n",
    "            main_category_link = category.find('a', class_='i-link')\n",
    "            svg_element = category.find('span', class_='svg-i svg-arrow')\n",
    "            if not main_category_link or not svg_element:\n",
    "                continue  \n",
    "\n",
    "            main_category_name = main_category_link.find('span', class_='lnk-text').get_text(strip=True)\n",
    "\n",
    "            subcategories = []\n",
    "            subcategories_container = category.find('div', class_='submenu')\n",
    "            if subcategories_container:\n",
    "                for subcategory in subcategories_container.find_all('li'):\n",
    "                    subcategory_link = subcategory.find('a', class_='i-link')\n",
    "                    if subcategory_link:\n",
    "                        subcategory_name = subcategory_link.find('span').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)  \n",
    "\n",
    "            categories_dict[main_category_name] = subcategories\n",
    "\n",
    "        return len(categories_dict), categories_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting categories: {e}\")\n",
    "        return 0, {}\n",
    "\n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.pcdiga.com' and (year not in category_analysis[category_analysis['site'] == site_column]['ano'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/') \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_pcdiga_20(link)\n",
    "                category_data.append({\n",
    "                    'ano': year,\n",
    "                    'link': link,\n",
    "                    'site': site_column,\n",
    "                    'numero_categorias': num_categories,\n",
    "                    'lista_categorias': list(category_dict.keys()),\n",
    "                    'dicionario_subcategorias': category_dict\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RadioPopular Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            for category in soup.find_all('td', align='left'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.find('img').get('alt')\n",
    "                    categories_dict[main_category_name] = []\n",
    "            for i in list(categories_dict.keys()):\n",
    "                if i == 'Recrutamento':\n",
    "                    del categories_dict[i]\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010 or year == 2007:\n",
    "                num_categories, category_dict = get_categories_2010_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            div = soup.find('div', id='nav')\n",
    "            if div is None:\n",
    "                return 0, {} \n",
    "\n",
    "            for category in div.find_all('li', class_='dir'):\n",
    "                main_category_name = category.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "\n",
    "                subcategories_container = category.find('ul')\n",
    "                if subcategories_container:\n",
    "                    for subcategory in subcategories_container.find_all('li', class_='dir'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "\n",
    "                if subcategories:\n",
    "                    categories_dict[main_category_name] = subcategories\n",
    "                else:\n",
    "                    categories_dict[main_category_name] = []\n",
    "\n",
    "            # vamos ver todas as keys que tem [] e eliminar, pois não tem subcategorias\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "                    \n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_rp(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            categories_dict = {}\n",
    "\n",
    "            # Encontrar o div com as categorias principais\n",
    "            ul = soup.find('ul', class_='categories')\n",
    "\n",
    "            for i in ul.find_all('li', class_='category link cb'):\n",
    "                main_category_name = i.find('a').get_text(strip=True)\n",
    "                subcategories = []\n",
    "                div = i.find('div', class_='subcategories')\n",
    "                if div:\n",
    "                    for subcategory in div.find_all('li', class_=\"subcategory family link\"):\n",
    "                        subcategory_name = subcategory.get_text(strip=True)\n",
    "                        subcategories.append(subcategory_name)\n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.radiopopular.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')    \n",
    "            if year == 2020 or year == 2023:\n",
    "                num_categories, category_dict = get_categories_2020_rp(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As elcorreioingles does not have product categories, we will not use this site to make this comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fnac Extracted Categories:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is not possible to extract categories, from 2006 and 2008, we will not use the year 2007 in fnac to make the comparison\n",
    "\n",
    "Error message presented:```O seu browser não aceita cookies, pelo que não é possível o acesso ao nosso site.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2010_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='menu')\n",
    "            u = div.find('ul', style=\"margin-left: 40px\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                ul = category.find('ul')\n",
    "                if ul:\n",
    "                    for subcategory in ul.find_all('li'):\n",
    "                        subcategory_name = subcategory.find('a').get_text(strip=True)\n",
    "                        subcategory_name = subcategory_name.replace('»', '').replace('\\r\\n', '').strip()\n",
    "                        subcategory_name = ' '.join(subcategory_name.split())\n",
    "                        subcategories.append(subcategory_name)\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "            for value in categories_dict.values():\n",
    "                if 'Ver todos os produtos' in value:\n",
    "                    value.remove('Ver todos os produtos')\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2010:\n",
    "                num_categories, category_dict = get_categories_2010_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2015_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', id='MENU')\n",
    "            u = div.find('ul', id=\"onglets\")\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "            for category in u.find_all('li'):\n",
    "                a = category.find('a')\n",
    "                if a:\n",
    "                    span = a.find('span', class_='inner')\n",
    "                    main_category_name = a.get_text(strip=True)\n",
    "                subcategories = []\n",
    "                \n",
    "                div = category.find('div', class_='megaMenu')\n",
    "                if div:\n",
    "                    for sub in div.find_all('dt'):\n",
    "                        # pode ja ter subcategoria ou ter um <a> com a subcategoria\n",
    "                        subcategory_name = sub.get_text(strip=True)\n",
    "                        if subcategory_name:\n",
    "                            subcategories.append(subcategory_name)\n",
    "\n",
    "                    if subcategories:\n",
    "                        categories_dict[main_category_name] = subcategories\n",
    "                    else:\n",
    "                        categories_dict[main_category_name] = []\n",
    "\n",
    "            for key in list(categories_dict.keys()):\n",
    "                if len(categories_dict[key]) == 0:\n",
    "                    del categories_dict[key]\n",
    "\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2015:\n",
    "                num_categories, category_dict = get_categories_2015_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2020_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='Sidebar-nano nano')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"CategoryNav js-CategoryNav\")\n",
    "            for il in ul.find_all('li', class_='CategoryNav-item js-CategoryNav-item'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2020:\n",
    "                num_categories, category_dict = get_categories_2020_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_2023_fnac(url):\n",
    "        try:\n",
    "            request = requests.get(url)\n",
    "            request.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(request.text, 'html.parser')\n",
    "            div = soup.find('div', class_='SideNavPanel-listWrapper')\n",
    "            categories_dict = {}\n",
    "            main_category_name = None\n",
    "\n",
    "            ul = div.find('ul', class_=\"SideNavPanel-list\")\n",
    "            for il in ul.find_all('li', class_='SideNavPanel-listItem js-SideNavPanel-listItem'):\n",
    "                main_category_name = il.find('a').get_text(strip=True)\n",
    "                subcategories = []  \n",
    "                categories_dict[main_category_name] = subcategories\n",
    "            return len(categories_dict), categories_dict\n",
    "        except Exception as e:\n",
    "            return 0, {}\n",
    "        \n",
    "category_data = []\n",
    "\n",
    "data = pd.read_csv('data/sites_links.csv')\n",
    "category_analysis = pd.read_csv('data/ecommerce_category_analysis_all.csv')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    year = row['Unnamed: 0']\n",
    "    \n",
    "    for site_column in data.columns[1:]:\n",
    "        link = row[site_column]\n",
    "        if pd.notna(link) and site_column == 'www.fnac.pt' and (link not in category_analysis['link'].values):\n",
    "            link = link.replace(f'/http://{site_column}/', f'mp_/http://{site_column}/')      \n",
    "            if year == 2023:\n",
    "\n",
    "                num_categories, category_dict = get_categories_2023_fnac(link)\n",
    "                if num_categories > 0:\n",
    "                    category_data.append({\n",
    "                        'ano': year,\n",
    "                        'link': link,\n",
    "                        'site': site_column,\n",
    "                        'numero_categorias': num_categories,\n",
    "                        'lista_categorias': list(category_dict.keys()),\n",
    "                        'dicionario_subcategorias': category_dict\n",
    "                    })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Adicionar os dados coletados ao csv\n",
    "category_data_df = pd.DataFrame(category_data)\n",
    "category_data_df.to_csv(\"data/ecommerce_category_analysis_all.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Extracting product prices\n",
    "\n",
    "We will use the BeautifulSoup library to extract the product prices from the websites, and we will use the requests library to get the HTML content of the websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/smartphones_arquivo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>model</th>\n",
       "      <th>periodo</th>\n",
       "      <th>title</th>\n",
       "      <th>originalURL</th>\n",
       "      <th>linkToArchive</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>contentLength</th>\n",
       "      <th>digest</th>\n",
       "      <th>mimeType</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>linkToScreenshot</th>\n",
       "      <th>linkToNoFrame</th>\n",
       "      <th>linkToExtractedText</th>\n",
       "      <th>linkToMetadata</th>\n",
       "      <th>linkToOriginalFile</th>\n",
       "      <th>snippet</th>\n",
       "      <th>fileName</th>\n",
       "      <th>collection</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.fnac.pt</td>\n",
       "      <td>huawei p8</td>\n",
       "      <td>2016-2020</td>\n",
       "      <td>Huawei P8 Lite Huawei - Tecnologia - Fnac.pt</td>\n",
       "      <td>https://www.fnac.pt/Smartphones-e-Telemoveis/S...</td>\n",
       "      <td>https://arquivo.pt/wayback/20170819112525/http...</td>\n",
       "      <td>20170819112525</td>\n",
       "      <td>188977</td>\n",
       "      <td>d129cd257066d54e046fd147e20e50a0</td>\n",
       "      <td>text/html</td>\n",
       "      <td>...</td>\n",
       "      <td>1503141925</td>\n",
       "      <td>https://arquivo.pt/screenshot?url=https%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201708191125...</td>\n",
       "      <td>https://arquivo.pt/textextracted?m=https%3A%2F...</td>\n",
       "      <td>https://arquivo.pt/textsearch?metadata=https%3...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201708191125...</td>\n",
       "      <td>&lt;em&gt;Huawei&lt;/em&gt; &lt;em&gt;P8&lt;/em&gt; Lite &lt;em&gt;Huawei&lt;/e...</td>\n",
       "      <td>IAH-20170819112306-68006-p81.arquivo.pt</td>\n",
       "      <td>AWP24</td>\n",
       "      <td>3773684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.fnac.pt</td>\n",
       "      <td>huawei p8</td>\n",
       "      <td>2016-2020</td>\n",
       "      <td>HUAWEI TELEMOVEL HUAWEI P8 DS 64GB, SmartPhone...</td>\n",
       "      <td>http://www.fnac.pt/HUAWEI-TELEMOVEL-HUAWEI-P8-...</td>\n",
       "      <td>https://arquivo.pt/wayback/20160211013245/http...</td>\n",
       "      <td>20160211013245</td>\n",
       "      <td>168566</td>\n",
       "      <td>e7cba63ab0ad2ff865ee01fc24aaeb21</td>\n",
       "      <td>text/html</td>\n",
       "      <td>...</td>\n",
       "      <td>1455154365</td>\n",
       "      <td>https://arquivo.pt/screenshot?url=https%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201602110132...</td>\n",
       "      <td>https://arquivo.pt/textextracted?m=http%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/textsearch?metadata=http%3A...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201602110132...</td>\n",
       "      <td>&lt;em&gt;HUAWEI&lt;/em&gt; TELEMOVEL &lt;em&gt;HUAWEI&lt;/em&gt; &lt;em&gt;...</td>\n",
       "      <td>IAH-20160211012526-43988-p81.arquivo.pt</td>\n",
       "      <td>AWP20</td>\n",
       "      <td>44697354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>www.fnac.pt</td>\n",
       "      <td>huawei p20</td>\n",
       "      <td>2016-2020</td>\n",
       "      <td>Smartphone Huawei P20 - 128GB - Black - SmartP...</td>\n",
       "      <td>https://www.fnac.pt/Smartphone-Huawei-P20-128G...</td>\n",
       "      <td>https://arquivo.pt/wayback/20190422222720/http...</td>\n",
       "      <td>20190422222720</td>\n",
       "      <td>311043</td>\n",
       "      <td>85b8ba9669ccbedeae1fcd8fe1d10abd</td>\n",
       "      <td>text/html</td>\n",
       "      <td>...</td>\n",
       "      <td>1555972040</td>\n",
       "      <td>https://arquivo.pt/screenshot?url=https%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201904222227...</td>\n",
       "      <td>https://arquivo.pt/textextracted?m=https%3A%2F...</td>\n",
       "      <td>https://arquivo.pt/textsearch?metadata=https%3...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201904222227...</td>\n",
       "      <td>Smartphone &lt;em&gt;Huawei&lt;/em&gt; &lt;em&gt;P20&lt;/em&gt; - 128G...</td>\n",
       "      <td>WEB-20190422222653112-p81.arquivo.pt</td>\n",
       "      <td>AWP29</td>\n",
       "      <td>63201824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.fnac.pt</td>\n",
       "      <td>huawei p20</td>\n",
       "      <td>2016-2020</td>\n",
       "      <td>Smartphone Huawei P20 Pro - 128GB - Black - Sm...</td>\n",
       "      <td>https://www.fnac.pt/Smartphone-Huawei-P20-Pro-...</td>\n",
       "      <td>https://arquivo.pt/wayback/20190422222711/http...</td>\n",
       "      <td>20190422222711</td>\n",
       "      <td>293053</td>\n",
       "      <td>2530c422432b25e633797fa5469eb6b5</td>\n",
       "      <td>text/html</td>\n",
       "      <td>...</td>\n",
       "      <td>1555972031</td>\n",
       "      <td>https://arquivo.pt/screenshot?url=https%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201904222227...</td>\n",
       "      <td>https://arquivo.pt/textextracted?m=https%3A%2F...</td>\n",
       "      <td>https://arquivo.pt/textsearch?metadata=https%3...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201904222227...</td>\n",
       "      <td>Smartphone &lt;em&gt;Huawei&lt;/em&gt; &lt;em&gt;P20&lt;/em&gt; Pro - ...</td>\n",
       "      <td>WEB-20190422222653112-p81.arquivo.pt</td>\n",
       "      <td>AWP29</td>\n",
       "      <td>19579119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>www.fnac.pt</td>\n",
       "      <td>huawei p30</td>\n",
       "      <td>2016-2020</td>\n",
       "      <td>Novos Huawei P30 - Sabe mais em Fnac.pt</td>\n",
       "      <td>https://www.fnac.pt/novos-huawei</td>\n",
       "      <td>https://arquivo.pt/wayback/20190323202914/http...</td>\n",
       "      <td>20190323202914</td>\n",
       "      <td>57729</td>\n",
       "      <td>38aaee64d7aa7b777d18759f0de09b53</td>\n",
       "      <td>text/html</td>\n",
       "      <td>...</td>\n",
       "      <td>1553372954</td>\n",
       "      <td>https://arquivo.pt/screenshot?url=https%3A%2F%...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201903232029...</td>\n",
       "      <td>https://arquivo.pt/textextracted?m=https%3A%2F...</td>\n",
       "      <td>https://arquivo.pt/textsearch?metadata=https%3...</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201903232029...</td>\n",
       "      <td>Novos &lt;em&gt;Huawei&lt;/em&gt; &lt;em&gt;P30&lt;/em&gt; - Sabe mais...</td>\n",
       "      <td>WEB-20190323202843497-p81.arquivo.pt</td>\n",
       "      <td>AWP29</td>\n",
       "      <td>87173605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          site       model    periodo  \\\n",
       "0  www.fnac.pt   huawei p8  2016-2020   \n",
       "1  www.fnac.pt   huawei p8  2016-2020   \n",
       "2  www.fnac.pt  huawei p20  2016-2020   \n",
       "3  www.fnac.pt  huawei p20  2016-2020   \n",
       "4  www.fnac.pt  huawei p30  2016-2020   \n",
       "\n",
       "                                               title  \\\n",
       "0       Huawei P8 Lite Huawei - Tecnologia - Fnac.pt   \n",
       "1  HUAWEI TELEMOVEL HUAWEI P8 DS 64GB, SmartPhone...   \n",
       "2  Smartphone Huawei P20 - 128GB - Black - SmartP...   \n",
       "3  Smartphone Huawei P20 Pro - 128GB - Black - Sm...   \n",
       "4            Novos Huawei P30 - Sabe mais em Fnac.pt   \n",
       "\n",
       "                                         originalURL  \\\n",
       "0  https://www.fnac.pt/Smartphones-e-Telemoveis/S...   \n",
       "1  http://www.fnac.pt/HUAWEI-TELEMOVEL-HUAWEI-P8-...   \n",
       "2  https://www.fnac.pt/Smartphone-Huawei-P20-128G...   \n",
       "3  https://www.fnac.pt/Smartphone-Huawei-P20-Pro-...   \n",
       "4                   https://www.fnac.pt/novos-huawei   \n",
       "\n",
       "                                       linkToArchive          tstamp  \\\n",
       "0  https://arquivo.pt/wayback/20170819112525/http...  20170819112525   \n",
       "1  https://arquivo.pt/wayback/20160211013245/http...  20160211013245   \n",
       "2  https://arquivo.pt/wayback/20190422222720/http...  20190422222720   \n",
       "3  https://arquivo.pt/wayback/20190422222711/http...  20190422222711   \n",
       "4  https://arquivo.pt/wayback/20190323202914/http...  20190323202914   \n",
       "\n",
       "   contentLength                            digest   mimeType  ...  \\\n",
       "0         188977  d129cd257066d54e046fd147e20e50a0  text/html  ...   \n",
       "1         168566  e7cba63ab0ad2ff865ee01fc24aaeb21  text/html  ...   \n",
       "2         311043  85b8ba9669ccbedeae1fcd8fe1d10abd  text/html  ...   \n",
       "3         293053  2530c422432b25e633797fa5469eb6b5  text/html  ...   \n",
       "4          57729  38aaee64d7aa7b777d18759f0de09b53  text/html  ...   \n",
       "\n",
       "         date                                   linkToScreenshot  \\\n",
       "0  1503141925  https://arquivo.pt/screenshot?url=https%3A%2F%...   \n",
       "1  1455154365  https://arquivo.pt/screenshot?url=https%3A%2F%...   \n",
       "2  1555972040  https://arquivo.pt/screenshot?url=https%3A%2F%...   \n",
       "3  1555972031  https://arquivo.pt/screenshot?url=https%3A%2F%...   \n",
       "4  1553372954  https://arquivo.pt/screenshot?url=https%3A%2F%...   \n",
       "\n",
       "                                       linkToNoFrame  \\\n",
       "0  https://arquivo.pt/noFrame/replay/201708191125...   \n",
       "1  https://arquivo.pt/noFrame/replay/201602110132...   \n",
       "2  https://arquivo.pt/noFrame/replay/201904222227...   \n",
       "3  https://arquivo.pt/noFrame/replay/201904222227...   \n",
       "4  https://arquivo.pt/noFrame/replay/201903232029...   \n",
       "\n",
       "                                 linkToExtractedText  \\\n",
       "0  https://arquivo.pt/textextracted?m=https%3A%2F...   \n",
       "1  https://arquivo.pt/textextracted?m=http%3A%2F%...   \n",
       "2  https://arquivo.pt/textextracted?m=https%3A%2F...   \n",
       "3  https://arquivo.pt/textextracted?m=https%3A%2F...   \n",
       "4  https://arquivo.pt/textextracted?m=https%3A%2F...   \n",
       "\n",
       "                                      linkToMetadata  \\\n",
       "0  https://arquivo.pt/textsearch?metadata=https%3...   \n",
       "1  https://arquivo.pt/textsearch?metadata=http%3A...   \n",
       "2  https://arquivo.pt/textsearch?metadata=https%3...   \n",
       "3  https://arquivo.pt/textsearch?metadata=https%3...   \n",
       "4  https://arquivo.pt/textsearch?metadata=https%3...   \n",
       "\n",
       "                                  linkToOriginalFile  \\\n",
       "0  https://arquivo.pt/noFrame/replay/201708191125...   \n",
       "1  https://arquivo.pt/noFrame/replay/201602110132...   \n",
       "2  https://arquivo.pt/noFrame/replay/201904222227...   \n",
       "3  https://arquivo.pt/noFrame/replay/201904222227...   \n",
       "4  https://arquivo.pt/noFrame/replay/201903232029...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  <em>Huawei</em> <em>P8</em> Lite <em>Huawei</e...   \n",
       "1  <em>HUAWEI</em> TELEMOVEL <em>HUAWEI</em> <em>...   \n",
       "2  Smartphone <em>Huawei</em> <em>P20</em> - 128G...   \n",
       "3  Smartphone <em>Huawei</em> <em>P20</em> Pro - ...   \n",
       "4  Novos <em>Huawei</em> <em>P30</em> - Sabe mais...   \n",
       "\n",
       "                                  fileName collection    offset  \n",
       "0  IAH-20170819112306-68006-p81.arquivo.pt      AWP24   3773684  \n",
       "1  IAH-20160211012526-43988-p81.arquivo.pt      AWP20  44697354  \n",
       "2     WEB-20190422222653112-p81.arquivo.pt      AWP29  63201824  \n",
       "3     WEB-20190422222653112-p81.arquivo.pt      AWP29  19579119  \n",
       "4     WEB-20190323202843497-p81.arquivo.pt      AWP29  87173605  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176 entries, 0 to 175\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   site                 176 non-null    object\n",
      " 1   model                176 non-null    object\n",
      " 2   periodo              176 non-null    object\n",
      " 3   title                176 non-null    object\n",
      " 4   originalURL          176 non-null    object\n",
      " 5   linkToArchive        176 non-null    object\n",
      " 6   tstamp               176 non-null    int64 \n",
      " 7   contentLength        176 non-null    int64 \n",
      " 8   digest               176 non-null    object\n",
      " 9   mimeType             176 non-null    object\n",
      " 10  encoding             176 non-null    object\n",
      " 11  date                 176 non-null    int64 \n",
      " 12  linkToScreenshot     176 non-null    object\n",
      " 13  linkToNoFrame        176 non-null    object\n",
      " 14  linkToExtractedText  176 non-null    object\n",
      " 15  linkToMetadata       176 non-null    object\n",
      " 16  linkToOriginalFile   176 non-null    object\n",
      " 17  snippet              176 non-null    object\n",
      " 18  fileName             176 non-null    object\n",
      " 19  collection           176 non-null    object\n",
      " 20  offset               176 non-null    int64 \n",
      "dtypes: int64(4), object(17)\n",
      "memory usage: 29.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices(text, site):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    price_container = soup.find(\"div\", id=\"ArticleHeaderPT_FullPricerControl_ctl00_BuyBoxMarketPlace_OfferPrice1\")\n",
    "    if price_container:\n",
    "        price_single = price_container.find(\"span\", class_=\"price\")\n",
    "        if price_single:\n",
    "            price = price_single.get_text(strip=True)\n",
    "            print('1.', price)\n",
    "            return {'current_price': price}\n",
    "        \n",
    "    price_container_box = soup.find(\"div\", class_=\"ProductPriceBox\")\n",
    "    if price_container_box:\n",
    "        price_single = price_container_box.find(\"strong\", class_=\"product-price\")\n",
    "        if price_single:\n",
    "            price = price_single.get_text(strip=True)\n",
    "            print('1.', price)\n",
    "            return {'current_price': price}\n",
    "\n",
    "    price_box = soup.find(\"div\", class_=\"f-priceBox\")\n",
    "    if price_box:\n",
    "        old_price = price_box.find(\"span\", class_=\"f-priceBox-price f-priceBox-price--old\")\n",
    "        current_price = price_box.find(\"span\", class_=\"f-priceBox-price f-priceBox-price--reco checked\")\n",
    "        if old_price and current_price:\n",
    "            print('2.',old_price.get_text(strip=True), current_price.get_text(strip=True))\n",
    "            return {'old_price': old_price.get_text(strip=True), 'current_price': current_price.get_text(strip=True)}\n",
    "        else:\n",
    "            print('3.',current_price.get_text(strip=True))\n",
    "            return {'current_price': current_price.get_text(strip=True)}\n",
    "    \n",
    "    product_list = soup.find_all(\"li\", class_=\"clearfix Article-item\") or soup.find_all(\"div\", class_=\"Article-itemGroup\")\n",
    "    if product_list:\n",
    "        products = []\n",
    "        for product in product_list:\n",
    "            name_tag = product.find(\"a\", class_=\"js-minifa-title\") or product.find(\"p\", class_=\"Article-desc\")\n",
    "            product_name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "            old_price_tag = product.find(\"span\", class_=\"oldPrice\")\n",
    "            current_price_tag = product.find(\"a\", class_=\"userPrice\") or product.find(\"strong\", class_=\"userPrice\")\n",
    "            if old_price_tag and current_price_tag:\n",
    "                old_price = old_price_tag.get_text(strip=True)\n",
    "                current_price = current_price_tag.get_text(strip=True)\n",
    "                print('4.',product_name, old_price, current_price)\n",
    "                products.append({\n",
    "                'product_name': product_name,\n",
    "                'old_price': old_price,\n",
    "                'current_price': current_price\n",
    "                })\n",
    "            elif current_price_tag and not old_price_tag:\n",
    "                old_price = None\n",
    "                current_price = current_price_tag.get_text(strip=True)\n",
    "                print('5.',product_name, current_price)\n",
    "                products.append({\n",
    "                    'product_name': product_name,\n",
    "                    'current_price': current_price\n",
    "                })\n",
    "        \n",
    "        return products if products else None\n",
    "\n",
    "    blk_inside = soup.find(\"div\", class_=\"blk_inside\")\n",
    "    if blk_inside:\n",
    "        products = []\n",
    "        for produit in blk_inside.find_all(\"div\", class_=\"produit\"):\n",
    "            name_tag = produit.find(\"dt\")\n",
    "            product_name = name_tag.get_text(strip=True) if name_tag else None\n",
    "            \n",
    "            price_tag = produit.find(\"span\", class_=\"smallPrice\")\n",
    "            price = price_tag.get_text(strip=True) if price_tag else None\n",
    "            print('6.',product_name, price)\n",
    "            products.append({\n",
    "                'product_name': product_name,\n",
    "                'current_price': price\n",
    "            })\n",
    "        \n",
    "        return products if products else None\n",
    "\n",
    "    product_list = soup.find_all(\"div\", class_=\"thumbnail\")\n",
    "    if product_list:\n",
    "        products = []\n",
    "        for product in product_list:\n",
    "            name_tag = product.find(\"a\", class_=\"thumbnail-titleLink\")\n",
    "            product_name = name_tag.get_text(strip=True) if name_tag else None\n",
    "            \n",
    "            price_tag = product.find(\"span\", class_=\"thumbnail-price\")\n",
    "            old_price_tag = product.find(\"del\", class_=\"thumbnail-priceOld\")\n",
    "            \n",
    "            current_price = price_tag.get_text(strip=True) if price_tag else None\n",
    "            old_price = old_price_tag.get_text(strip=True) if old_price_tag else None\n",
    "\n",
    "            if current_price and old_price:\n",
    "                print('7.',product_name, old_price, current_price)\n",
    "                products.append({\n",
    "                    'product_name': product_name,\n",
    "                    'old_price': old_price,\n",
    "                    'current_price': current_price\n",
    "                })\n",
    "            elif current_price and not old_price:\n",
    "                print('8.',product_name, current_price)\n",
    "                products.append({\n",
    "                    'product_name': product_name,\n",
    "                    'current_price': current_price\n",
    "                })\n",
    "        return products if products else None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_FNAC(df):\n",
    "    extracted_data_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        link = row['linkToArchive']\n",
    "        site = row['site'] \n",
    "        link = link.replace(f'/https://{site}/', f'mp_/https://{site}/')\n",
    "        tstamp = str(row['tstamp'])\n",
    "        date = tstamp[:8]\n",
    "        title = row['title']\n",
    "\n",
    "        \n",
    "        if site == 'www.fnac.pt':\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                response.raise_for_status()\n",
    "                text = response.text\n",
    "            except requests.RequestException:\n",
    "                print(f\"Error while fetching {link}\")\n",
    "                extracted_data_list.append({\n",
    "                            'site': site,\n",
    "                            'date': date,\n",
    "                            'title': titulo,\n",
    "                            'extractedData': None,\n",
    "                            'linkToArchive': link,\n",
    "                            'linkToExtractedText': row['linkToExtractedText']\n",
    "                        })\n",
    "            \n",
    "            extracted_data = extract_prices(text, site)\n",
    "            \n",
    "            if extracted_data:\n",
    "                if isinstance(extracted_data, list):\n",
    "                    for product in extracted_data:\n",
    "                        old = product['old_price'] if 'old_price' in product else None\n",
    "                        current = product['current_price'] if 'current_price' in product else None\n",
    "                        price = [current, old] if old and current else [current]\n",
    "                        titulo = product['product_name'] if 'product_name' in product else None\n",
    "                        extracted_data_list.append({\n",
    "                            'site': site,\n",
    "                            'date': date,\n",
    "                            'title': titulo,\n",
    "                            'extractedData': price,\n",
    "                            'linkToArchive': link,\n",
    "                            'linkToExtractedText': row['linkToExtractedText']\n",
    "                        })\n",
    "                else:\n",
    "                    old = extracted_data['old_price'] if 'old_price' in extracted_data else None\n",
    "                    current = extracted_data['current_price'] if 'current_price' in extracted_data else None\n",
    "                    price = [current, old] if old and current else [current]\n",
    "                    \n",
    "                    extracted_data_list.append({\n",
    "                        'site': site,\n",
    "                        'date': date,\n",
    "                        'title': title,\n",
    "                        'extractedData': price,\n",
    "                        'linkToArchive': link,\n",
    "                        'linkToExtractedText': row['linkToExtractedText']\n",
    "                    })\n",
    "\n",
    "            else:\n",
    "                extracted_data_list.append({\n",
    "                    'site': site,\n",
    "                    'date': date,\n",
    "                    'title': title,\n",
    "                    'extractedData': None,\n",
    "                    'linkToArchive': link,\n",
    "                    'linkToExtractedText': row['linkToExtractedText']\n",
    "                })\n",
    "    \n",
    "    return extracted_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Huawei P8 Lite 2017 - Black 259,99 € 219,99 €\n",
      "4. Huawei P8 Lite (Black) 229,99 € 179,99 €\n",
      "4. Huawei P8 Lite (Gold) 229,99 € 179,99 €\n",
      "4. Huawei P8 Lite 2017 - Gold 259,99 € 219,99 €\n",
      "4. Huawei P8 Lite (White) 229,99 € 179,99 €\n",
      "4. Huawei P8 Lite 2017 - White 259,99 € 219,99 €\n",
      "5. Huawei P8 Lite 16GB 4G Preto 169,90 €\n",
      "5. Smartphone Huawei P8 Lite Dual Sim 2GB 16GB Dourado 179,30 €\n",
      "5. Huawei P8 Lite 16GB 4G Branco 169,93 €\n",
      "5. Huawei P8 Lite 2017 Dual SIM 4G 16GB Azul 286,68 €\n",
      "5. smartphone Huawei P8 Lite 772997 4G 16GB Preto 283 €\n",
      "5. Huawei P8 Lite 2017 4G 16GB Dourado 278 €\n",
      "5. Huawei P8 Lite 2017 4G 16GB 278 €\n",
      "5. Huawei P8 Lite 2017 4G 16GB 278 €\n",
      "5. Smartphone HUAWEI P8 Lite 4G 16GB (Preto) 172,90 €\n",
      "5. Huawei P8 Lite 4G 16GB Branco 169,90 €\n",
      "5. Huawei P8 Lite 16GB 4G Preto 169,90 €\n",
      "3. 599,99 €\n",
      "3. 799,99 €\n",
      "2. 379,99 € 349,99 €\n",
      "3. 1 049,99 €\n",
      "3. 1 049,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Preto Meia-Noite 719,99 € 549,99 €\n",
      "4. Samsung Galaxy S8+ - G955FZ - Prateado 819,99 € 649,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Prateado 719,99 € 549,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Cinzento Orquídea 719,99 € 549,99 €\n",
      "5. Samsung Galaxy S8+ - G955FZ - Preto Meia-Noite 469,90 €\n",
      "5. Samsung Galaxy S8 Dual Sim- G950F-DS - Preto Meia-Noite 442,28 €\n",
      "5. Samsung Galaxy S8+ - G955FD - Preto Meia-Noite 458,80 €\n",
      "5. Samsung Galaxy S8+ Dual Sim - G955F/DS - Maple Dourado 458 €\n",
      "5. Samsung Galaxy S8 - G950FZ - Rosa 546 €\n",
      "5. Samsung Galaxy S8+ - G955FZ - Cinzento Orquídea 471,10 €\n",
      "5. Smartphone Samsung Galaxy S8+ G955Fds Dual SIM 4GB 64GB Cinzento Orquídea 478 €\n",
      "5. Samsung Galaxy S8 PLUS SM-G955F 4GB RAM 64 GB Preto 475,99 €\n",
      "5. Smartphone Samsung Galaxy S8 - 64GB - Blue Coral 516,89 €\n",
      "5. Smartphone Samsung Galaxy S8 G950F - 64GB - Preto Meia-Noite 533,49 €\n",
      "5. Smartphone Samsung Galaxy S8 G950F - 64GB - Coral Blue 438,10 €\n",
      "5. Smartphone Samsung Galaxy S8 Plus Dual SIM 6GB 128GB Cinzento Orquídea 719,99 €\n",
      "5. Smartphone Samsung Galaxy S8 G950Fds Dual SIM 4GB 64GB Coral Azul 458,89 €\n",
      "5. Smartphone Samsung Galaxy S8 G950Fds Dual SIM 4GB 64GB Cinzento Orquídea 444,80 €\n",
      "5. Smartphone TIM Samsung Galaxy S8 4GB 64 GB Azul 583 €\n",
      "5. Smartphone Telekom Samsung Galaxy S8 Plus 4GB 64 GB Cinzento 475,99 €\n",
      "4. Samsung Galaxy S8+ - G955FZ - Prateado 819,99 € 679,99 €\n",
      "4. Samsung Galaxy S8+ - G955FZ - Preto Meia-Noite 819,99 € 679,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Rosa 719,99 € 589,99 €\n",
      "5. Samsung Galaxy S8+ - G955FD - Preto Meia-Noite 819,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Cinzento Orquídea 719,99 € 589,99 €\n",
      "5. Samsung Galaxy S8+ Dual Sim - G955F/DS - Maple Dourado 819,99 €\n",
      "4. Samsung Galaxy S8 - G950FZ - Prateado 719,99 € 589,99 €\n",
      "5. Samsung Galaxy S8 Dual Sim- G950F-DS - Preto Meia-Noite 477,99 €\n",
      "5. Samsung Galaxy S8+ - G955FZ - Cinzento Orquídea 528 €\n",
      "5. Samsung Galaxy S8 - G950FZ - Preto Meia-Noite 475,90 €\n",
      "5. Smartphone Samsung Galaxy S8 - 64GB - Blue Coral 549,89 €\n",
      "5. Smartphone Samsung Galaxy S8 G950F - 64GB - Preto Meia-Noite 604,49 €\n",
      "5. Smartphone TIM Samsung Galaxy S8 4GB 64 GB Azul 570 €\n",
      "5. Smartphone Telekom Samsung Galaxy S8 Plus 4GB 64 GB Cinzento 493,99 €\n",
      "5. Smartphone Telekom Samsung Galaxy S8 Plus 4GB 64 GB Preto 661 €\n",
      "5. Smartphone Telecom Italia Samsung Galaxy S8+ 4GB 64 GB Cinzento 775 €\n",
      "5. Smartphone Telecom Italia Samsung Galaxy S8+ 4GB 64 GB Prateado 765 €\n",
      "5. Smartphone Telecom Italia Samsung Galaxy S8 4GB 64 GB Cinzento 570 €\n",
      "5. Smartphone Telecom Italia Samsung Galaxy S8 4GB 64 GB Prateado 570 €\n",
      "5. Samsung Galaxy S8 PLUS SM-G955F 4GB RAM 64 GB Preto 528 €\n",
      "8. Samsung Galaxy S9+ - G965FZ - Azul Topázio 969,99 €\n",
      "8. Samsung Galaxy S9+ - G965FZ - Rosa Púrpura 969,99 €\n",
      "8. Samsung Galaxy S9+ - G965FZ - Dourado Amanhecer 969,99 €\n",
      "8. Samsung Galaxy S9+ - G965FZ - Preto Meia-Noite 969,99 €\n",
      "8. Samsung Galaxy S9+ - G965FZ - 256GB - Cinzento Titânio 1 099,99 €\n",
      "8. Samsung Galaxy S9 - G960FZ - Rosa Púrpura 869,99 €\n",
      "8. Samsung Galaxy S9 - G960FZ - Azul Topázio 869,99 €\n",
      "8. Samsung Galaxy S9 - G960FZ - Preto Meia-Noite 869,99 €\n",
      "8. Samsung Galaxy S9 - G960FZ - 256GB - Cinzento Titânio 999,99 €\n",
      "8. Samsung Galaxy S9 - G960FZ - Dourado Amanhecer 869,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S9+ - Preto 59,99 €\n",
      "8. Capa Samsung Led View para Galaxy S9+ - Preto 69,99 €\n",
      "8. Capa Samsung Protective para Galaxy S9+ - Preto 39,99 €\n",
      "8. Capa Book Window 4-OK para Galaxy S9+ - Preto 19,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S9+ - Dourado 59,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S9 - Preto 59,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S9 - Preto 29,99 €\n",
      "8. Capa Book Window 4-OK para Galaxy S9 - Preto 19,99 €\n",
      "8. Capa 4-OK Ultra Slim para Galaxy S9 13,99 €\n",
      "8. Capa Samsung Led View para Galaxy S9 - Preto 69,99 €\n",
      "8. Película Ecrã Vidro Temperado 4-OK para Galaxy S9+ 19,99 €\n",
      "8. Película Ecrã Vidro Temperado 4-OK para Galaxy S9 19,99 €\n",
      "8. Película de Vidro Temperado Advansia para Samsung Galaxy S9 Plus Proteção Total 7,90 €\n",
      "8. Película Vidro Temperado 3D Multishop para Samsung Galaxy S9+ / S9 Plus 8,99 €\n",
      "8. Película Vidro Temperado Multishop para Samsung Galaxy S9+ / S9 Plus 6,99 €\n",
      "3. 969,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Verde 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 1TB - Branco Cerâmico 1 599,99 € 1 549,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Preto 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 512GB - Preto Cerâmico 1 199,99 € 1 149,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Branco 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Prateado Prisma 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 512GB - Branco Cerâmico 1 199,99 € 1 149,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 1TB - Preto Cerâmico 1 599,99 € 1 549,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Verde 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Branco 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Preto 849,99 € 749,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Prateado Prisma 849,99 € 749,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Preto 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Branco 849,99 € 749,99 €\n",
      "8. Samsung Galaxy S10 - G973FZ - 128GB - Verde 649,98 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Branco 699,99 € 679,99 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Preto 699,99 € 679,99 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Verde 699,99 € 679,99 €\n",
      "8. Samsung Galaxy S10e - G970FZ - Amarelo 500,85 €\n",
      "8. Smartphone Samsung Galaxy S10E 5.8\"\" 128Gb Dual Sim Branco 500,85 €\n",
      "8. Capa Samsung Clear para Galaxy S10+ - Transparente 19,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Preto 54,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Preto 29,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10+ - Preto 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Branco 29,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Verde 29,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10+ - Transparente 19,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Verde 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Branco 54,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10+ - Branco 69,99 €\n",
      "8. Capa Samsung Clear para Galaxy S10 - Transparente 19,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10e - Preto 19,99 €\n",
      "8. Película Ecrã Samsung para Galaxy S10 19,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Preto 29,99 €\n",
      "8. Capa Samsung Protective para Galaxy S10+ - Preto 39,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10 - Preto 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10 - Verde 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10e - Transparente 19,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Azul 29,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Branco 29,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10e - Preto 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10e - Verde 29,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Branco 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Preto 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10 - Preto 19,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10e - Branco 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10e - Branco 29,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Verde 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10 - Transparente 19,99 €\n",
      "8. Capa Samsung Pele para Galaxy S10e - Preto 49,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Verde 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 1TB - Branco Cerâmico 1 599,99 € 1 549,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Preto 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 512GB - Preto Cerâmico 1 199,99 € 1 149,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Branco 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FZ - 128GB - Prateado Prisma 949,99 € 899,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 512GB - Branco Cerâmico 1 199,99 € 1 149,99 €\n",
      "7. Samsung Galaxy S10+ - G975FC - 1TB - Preto Cerâmico 1 599,99 € 1 549,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Verde 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Branco 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Preto 849,99 € 749,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Prateado Prisma 849,99 € 749,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 512GB - Preto 949,99 € 849,99 €\n",
      "7. Samsung Galaxy S10 - G973FZ - 128GB - Branco 849,99 € 749,99 €\n",
      "8. Samsung Galaxy S10 - G973FZ - 128GB - Verde 649,98 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Branco 699,99 € 679,99 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Preto 699,99 € 679,99 €\n",
      "7. Samsung Galaxy S10e - G970FZ - Verde 699,99 € 679,99 €\n",
      "8. Samsung Galaxy S10e - G970FZ - Amarelo 500,85 €\n",
      "8. Smartphone Samsung Galaxy S10E 5.8\"\" 128Gb Dual Sim Branco 500,85 €\n",
      "8. Capa Samsung Clear para Galaxy S10+ - Transparente 19,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Preto 54,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Preto 29,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10+ - Preto 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Branco 29,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10+ - Verde 29,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10+ - Transparente 19,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Verde 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10+ - Branco 54,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10+ - Branco 69,99 €\n",
      "8. Capa Samsung Clear para Galaxy S10 - Transparente 19,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10e - Preto 19,99 €\n",
      "8. Película Ecrã Samsung para Galaxy S10 19,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Preto 29,99 €\n",
      "8. Capa Samsung Protective para Galaxy S10+ - Preto 39,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10 - Preto 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10 - Verde 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10e - Transparente 19,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Azul 29,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10 - Branco 29,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10e - Preto 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10e - Verde 29,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Branco 54,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Preto 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10 - Preto 19,99 €\n",
      "8. Capa Samsung Led View para Galaxy S10e - Branco 69,99 €\n",
      "8. Capa Samsung Silicone para Galaxy S10e - Branco 29,99 €\n",
      "8. Capa Samsung Clear View para Galaxy S10e - Verde 54,99 €\n",
      "8. Capa Itskins Spectrumclear para Samsung Galaxy S10 - Transparente 19,99 €\n",
      "8. Capa Samsung Pele para Galaxy S10e - Preto 49,99 €\n",
      "1. 779,99 €\n",
      "5. Apple iPhone 8 - 256GB - Dourado 889,99 €\n",
      "5. Apple iPhone 8 - 64GB - Cinzento Sideral 719,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Cinzento Sideral 829,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Dourado 829,99 €\n",
      "5. Apple iPhone 8 - 64GB - Prateado 719,99 €\n",
      "5. Apple iPhone 8 - 64GB - Dourado 719,99 €\n",
      "5. Apple iPhone 8 - 256GB - Cinzento Sideral 889,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Prateado 999,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Prateado 829,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Cinzento Sideral 999,99 €\n",
      "5. Apple iPhone 8 - 256GB - Prateado 889,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Dourado 999,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - (Product) Red - Edição Especial 744,90 €\n",
      "5. Apple iPhone 8 - 64GB - (Product) Red - Edição Especial 641,90 €\n",
      "5. Apple iPhone 8 - 256GB - (Product) Red - Edição Especial 768 €\n",
      "5. Apple iPhone 8 Plus - 256GB - (Product) Red - Edição Especial 888 €\n",
      "5. Apple iPhone 8 - 64GB - Cinzento Sideral 829,99 €\n",
      "5. Apple iPhone 8 - 64GB - (Product) Red - Edição Especial 665 €\n",
      "5. Apple iPhone 8 - 64GB - Dourado 829,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - (Product) Red - Edição Especial 781 €\n",
      "5. Apple iPhone 8 - 64GB - Prateado 829,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Cinzento Sideral 939,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Dourado 939,99 €\n",
      "5. Apple iPhone 8 Plus - 64GB - Prateado 939,99 €\n",
      "5. Apple iPhone 8 - 256GB - Cinzento Sideral 842,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Dourado 1 119,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Cinzento Sideral 1 119,99 €\n",
      "5. Apple iPhone 8 Plus - 256GB - Prateado 1 138,40 €\n",
      "5. Apple iPhone 8 Plus - 256GB - (Product) Red - Edição Especial 1 186 €\n",
      "5. Apple iPhone 8 - 256GB - (Product) Red - Edição Especial 1 009,99 €\n",
      "5. Apple iPhone 8 - 256GB - Prateado 1 009,99 €\n",
      "5. Apple iPhone 8 - 256GB - Dourado 1 009,99 €\n",
      "5. Apple iPhone X - 256GB - Cinzento Sideral 1 229,99 €\n",
      "5. Apple iPhone X - 256GB - Prateado 1 229,99 €\n",
      "5. Apple iPhone X - 64GB - Cinzento Sideral 1 059,99 €\n",
      "5. Apple iPhone X - 64GB - Prateado 1 059,99 €\n",
      "4. Apple iPhone X - 64GB - Prateado 1 179,99 € 1 029,99 €\n",
      "5. Apple iPhone X - 64GB - Cinzento Sideral 993,10 €\n",
      "5. Apple iPhone X 64GB Prateado 910 €\n",
      "5. Apple iPhone X 256GB Cinzento 1 060 €\n",
      "5. Apple iPhone X 64GB Cinzento 910 €\n",
      "5. Apple iPhone X 256GB Prateado 1 060 €\n",
      "5. Apple iPhone XR - 64GB - Preto 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Branco 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Amarelo 879,99 €\n",
      "5. Apple iPhone XR - 128GB - Preto 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Preto 1 049,99 €\n",
      "5. Apple iPhone XR - 128GB - Branco 939,99 €\n",
      "5. Apple iPhone XR - 128GB - Azul 939,99 €\n",
      "5. Apple iPhone XR - 64GB - Azul 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Coral 879,99 €\n",
      "5. Apple iPhone XR - 64GB - (Product) Red 879,99 €\n",
      "5. Apple iPhone XR - 128GB - Amarelo 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Coral 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - Branco 1 049,99 €\n",
      "5. Apple iPhone XR - 128GB - Coral 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Azul 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - Amarelo 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - (Product) Red 979,99 €\n",
      "5. Apple iPhone XR - 128GB - (Product) Red 846,87 €\n",
      "5. Apple iPhone XR 128GB Vermelho 796 €\n",
      "5. Apple iPhone XR 64GB Coral 738 €\n",
      "5. Apple iPhone X - 64GB - Prateado 1 179,99 €\n",
      "5. Apple iPhone X - 64GB - Cinzento Sideral 1 179,99 €\n",
      "5. Apple iPhone X - 256GB - Cinzento Sideral 1 359,99 €\n",
      "5. Apple iPhone X - 64GB - Cinzento Sideral 1 179,99 €\n",
      "5. Apple iPhone X - 256GB - Prateado 1 102,50 €\n",
      "5. Apple iPhone X - 64GB - Prateado 1 009 €\n",
      "5. Apple iPhone X 64GB Cinzento 1 009 €\n",
      "5. Apple iPhone X 256GB Cinzento 1 199 €\n",
      "5. Apple iPhone X 256GB Prateado 1 148,90 €\n",
      "5. Apple iPhone X 64GB Prateado 999 €\n",
      "5. Apple iPhone XR 64GB Azul 896,30 €\n",
      "5. Apple iPhone XR 64GB Amarelo 893,50 €\n",
      "5. Apple iPhone XR 64GB Preto 770 €\n",
      "5. Apple iPhone XR 256GB Azul 839 €\n",
      "5. Apple iPhone XR 128GB Azul 802 €\n",
      "5. Apple iPhone XR 256GB Coral 940 €\n",
      "5. Apple iPhone XR 128GB Amarelo 797 €\n",
      "5. Apple iPhone XR 256GB Vermelho 1 060,70 €\n",
      "5. Apple iPhone XR 256GB Preto 1 155,69 €\n",
      "5. Apple iPhone XR 128GB Preto 807 €\n",
      "5. Apple iPhone XR 256GB Branco 1 060,70 €\n",
      "5. Apple iPhone XR 128GB Branco 810 €\n",
      "5. Apple iPhone XR 64GB Vermelho 769 €\n",
      "5. Apple iPhone XR 64GB Amarelo 747 €\n",
      "5. Apple iPhone XR 64GB Azul 747 €\n",
      "5. Apple iPhone XR 64GB Preto 769 €\n",
      "5. Apple iPhone XR 64GB Branco 765 €\n",
      "5. Apple iPhone XS - 64GB - Dourado 1 179,99 €\n",
      "5. Apple iPhone XS - 64GB - Prateado 1 179,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Dourado 1 279,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Cinzento Sideral 1 279,99 €\n",
      "5. Apple iPhone XS - 64GB - Cinzento Sideral 1 179,99 €\n",
      "5. Apple iPhone XS - 256GB - Cinzento Sideral 1 349,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Prateado 1 449,99 €\n",
      "5. Apple iPhone XS - 256GB - Prateado 1 349,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Cinzento Sideral 1 449,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Dourado 1 449,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Prateado 1 279,99 €\n",
      "5. Apple iPhone XS - 512GB - Dourado 1 579,99 €\n",
      "5. Apple iPhone XS - 256GB - Dourado 1 349,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Dourado 1 679,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Prateado 1 679,99 €\n",
      "5. Apple iPhone XS - 512GB - Cinzento Sideral 1 240 €\n",
      "5. Apple iPhone XS - 512GB - Prateado 1 579,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Cinzento Sideral 1 679,99 €\n",
      "5. Apple iPhone XS 512GB Dourado 1 602,80 €\n",
      "5. Apple iPhone XS Max 64GB Dourado 1 129,85 €\n",
      "5. Apple iPhone XS Max 64GB Cinzento 1 096 €\n",
      "5. Apple iPhone XS 64GB Cinzento 1 006,73 €\n",
      "5. Apple iPhone XS Max 256GB Dourado 1 285,61 €\n",
      "5. Apple iPhone XS Max 256GB Prateado 1 250,21 €\n",
      "5. Apple iPhone XS Max 64GB Prateado 1 117,88 €\n",
      "5. Apple iPhone XS 64GB Prateado 984,71 €\n",
      "5. Apple iPhone XS 64GB Dourado 1 011 €\n",
      "5. Apple iPhone XS 256GB Dourado 1 167,61 €\n",
      "5. Apple iPhone XS Max 512GB Dourado 1 795,90 €\n",
      "5. Apple iPhone XS Max 512GB Prateado 1 565 €\n",
      "5. Apple iPhone XS Max 512GB Cinzento 1 565 €\n",
      "5. Apple iPhone XS Max 256GB Dourado 1 355 €\n",
      "5. Apple iPhone XS Max 256GB Prateado 1 471,10 €\n",
      "5. Apple iPhone XS Max 256GB Cinzento 1 482,10 €\n",
      "5. Apple iPhone XS 512GB Prateado 1 602,80 €\n",
      "5. Apple iPhone XS 512GB Cinzento 1 309,88 €\n",
      "5. Apple iPhone XS 256GB Dourado 1 382,20 €\n",
      "5. Apple iPhone XS 256GB Prateado 1 474 €\n",
      "5. Apple iPhone XS 256GB Cinzento 1 484,80 €\n",
      "5. Apple iPhone XS Max 64GB Cinzento 1 117,42 €\n",
      "3. 1 049,99 €\n",
      "3. 1 179,99 €\n",
      "5. Apple iPhone XS - 64GB - Dourado 1 179,99 €\n",
      "5. Apple iPhone XS - 64GB - Prateado 1 179,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Dourado 1 279,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Cinzento Sideral 1 279,99 €\n",
      "5. Apple iPhone XS - 64GB - Cinzento Sideral 1 179,99 €\n",
      "5. Apple iPhone XS - 256GB - Cinzento Sideral 1 349,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Prateado 1 449,99 €\n",
      "5. Apple iPhone XS - 256GB - Prateado 1 349,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Cinzento Sideral 1 449,99 €\n",
      "5. Apple iPhone XS Max- 256GB - Dourado 1 449,99 €\n",
      "5. Apple iPhone XS Max- 64GB - Prateado 1 279,99 €\n",
      "5. Apple iPhone XS - 512GB - Dourado 1 579,99 €\n",
      "5. Apple iPhone XS - 256GB - Dourado 1 349,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Dourado 1 679,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Prateado 1 679,99 €\n",
      "5. Apple iPhone XS - 512GB - Cinzento Sideral 1 240 €\n",
      "5. Apple iPhone XS - 512GB - Prateado 1 579,99 €\n",
      "5. Apple iPhone XS Max- 512GB - Cinzento Sideral 1 679,99 €\n",
      "5. Apple iPhone XS 512GB Dourado 1 602,80 €\n",
      "5. Apple iPhone XS Max 64GB Dourado 1 129,85 €\n",
      "5. Apple iPhone XS Max 64GB Cinzento 1 096 €\n",
      "5. Apple iPhone XS 64GB Cinzento 1 006,73 €\n",
      "5. Apple iPhone XS Max 256GB Dourado 1 285,61 €\n",
      "5. Apple iPhone XS Max 256GB Prateado 1 250,21 €\n",
      "5. Apple iPhone XS Max 64GB Prateado 1 117,88 €\n",
      "5. Apple iPhone XS 64GB Prateado 984,71 €\n",
      "5. Apple iPhone XS 64GB Dourado 1 011 €\n",
      "5. Apple iPhone XS 256GB Dourado 1 167,61 €\n",
      "5. Apple iPhone XS Max 512GB Dourado 1 795,90 €\n",
      "5. Apple iPhone XS Max 512GB Prateado 1 565 €\n",
      "5. Apple iPhone XS Max 512GB Cinzento 1 565 €\n",
      "5. Apple iPhone XS Max 256GB Dourado 1 355 €\n",
      "5. Apple iPhone XS Max 256GB Prateado 1 471,10 €\n",
      "5. Apple iPhone XS Max 256GB Cinzento 1 482,10 €\n",
      "5. Apple iPhone XS 512GB Prateado 1 602,80 €\n",
      "5. Apple iPhone XS 512GB Cinzento 1 309,88 €\n",
      "5. Apple iPhone XS 256GB Dourado 1 382,20 €\n",
      "5. Apple iPhone XS 256GB Prateado 1 474 €\n",
      "5. Apple iPhone XS 256GB Cinzento 1 484,80 €\n",
      "5. Apple iPhone XS Max 64GB Cinzento 1 117,42 €\n",
      "3. 1 179,99 €\n",
      "3. 1 349,99 €\n",
      "5. Apple iPhone XR - 64GB - Preto 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Branco 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Amarelo 879,99 €\n",
      "5. Apple iPhone XR - 128GB - Preto 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Preto 1 049,99 €\n",
      "5. Apple iPhone XR - 128GB - Branco 939,99 €\n",
      "5. Apple iPhone XR - 128GB - Azul 939,99 €\n",
      "5. Apple iPhone XR - 64GB - Azul 879,99 €\n",
      "5. Apple iPhone XR - 64GB - Coral 879,99 €\n",
      "5. Apple iPhone XR - 64GB - (Product) Red 879,99 €\n",
      "5. Apple iPhone XR - 128GB - Amarelo 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Coral 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - Branco 1 049,99 €\n",
      "5. Apple iPhone XR - 128GB - Coral 939,99 €\n",
      "5. Apple iPhone XR - 256GB - Azul 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - Amarelo 1 049,99 €\n",
      "5. Apple iPhone XR - 256GB - (Product) Red 979,99 €\n",
      "5. Apple iPhone XR - 128GB - (Product) Red 846,87 €\n",
      "5. Apple iPhone XR 128GB Vermelho 796 €\n",
      "5. Apple iPhone XR 64GB Coral 738 €\n",
      "5. Apple iPhone XR 64GB Azul 896,30 €\n",
      "5. Apple iPhone XR 64GB Amarelo 893,50 €\n",
      "5. Apple iPhone XR 64GB Preto 770 €\n",
      "5. Apple iPhone XR 256GB Azul 839 €\n",
      "5. Apple iPhone XR 128GB Azul 802 €\n",
      "5. Apple iPhone XR 256GB Coral 940 €\n",
      "5. Apple iPhone XR 128GB Amarelo 797 €\n",
      "5. Apple iPhone XR 256GB Vermelho 1 060,70 €\n",
      "5. Apple iPhone XR 256GB Preto 1 155,69 €\n",
      "5. Apple iPhone XR 128GB Preto 807 €\n",
      "5. Apple iPhone XR 256GB Branco 1 060,70 €\n",
      "5. Apple iPhone XR 128GB Branco 810 €\n",
      "5. Apple iPhone XR 64GB Vermelho 769 €\n",
      "5. Apple iPhone XR 64GB Amarelo 747 €\n",
      "5. Apple iPhone XR 64GB Azul 747 €\n",
      "5. Apple iPhone XR 64GB Preto 769 €\n",
      "5. Apple iPhone XR 64GB Branco 765 €\n",
      "3. 1 049,99 €\n",
      "5. Consola Sony PS4 Slim 1TB + Uncharted 4 + Horizon Zero Dawn 369,99 €\n",
      "5. Consola Sony PS4 Slim 1TB + Uncharted 4 + GTA V 369,99 €\n",
      "5. Consola Sony PS4 Slim 1TB + Ratchet & Clank + Crash Bandicoot N. Sane Trilogy 349,99 €\n",
      "4. Consola Sony PS4 Slim 500GB Dourada (Gold) - Edição Especial 2 Comandos 389,99 € 349,99 €\n",
      "5. Consola Sony PS4 Pro 1TB + Voucher PlayStation 10€ 399,99 €\n",
      "5. Consola Sony PS4 Slim 500GB Preta 299,99 €\n",
      "5. Consola Sony PS4 Slim 1TB 478,99 €\n",
      "5. Consola Sony PS4 Slim 500GB Prateada (Sliver) - Edição Especial 2 Comandos 398,99 €\n",
      "5. Pack Consola Sony PS4 Slim 1TB + Ratchet & Clank + Crash Bandicoot N. Sane Trilogy + FIFA 17 399,99 €\n",
      "5. Pack Consola Sony PS4 Pro 1TB + GTA V 429,90 €\n",
      "5. Consola Sony PS4 Slim 500GB Branca / Glacier White 342,03 €\n",
      "5. Consola Sony PS4 Slim 1TB + The Last Guardian 399,99 €\n",
      "5. Consola Sony PS4 Slim 1TB + Final Fantasy XV - Edição Limitada 499,99 €\n",
      "5. Consola Sony PS4 Pro 1TB 399 €\n",
      "5. Consola Sony PS4 Slim 1TB + Call of Duty: Infinite Warfare 379,99 €\n",
      "5. Consola Sony PS4 Slim 1TB + Mafia III 369,99 €\n",
      "5. Pack Consola Sony PS4 Slim 500GB + Pro Evolution Soccer 2017 344,99 €\n",
      "5. Consola Sony PS4 1TB + Street Fighter V 369,99 €\n",
      "5. Ni no Kuni II: Revenant Kingdom: Princes's Edition PS4 94,99 €\n",
      "5. Ni no Kuni II: Revenant Kingdom: King's Edition PS4 159,99 €\n",
      "5. Destiny 2 Collector's Edition PS4 249,99 €\n",
      "5. Star Wars: Battlefront II: Elite Trooper Deluxe Edition PS4 89,99 €\n",
      "5. Thrustmaster Volante TX Ferrari 458 Racing Xbox One / PC 349,99 €\n",
      "5. Thrustmaster Volante TMX Force Feedback - Xbox One 175,65 €\n",
      "5. Thrustmaster TX Racing Wheel Leather Edition Xbox One 399,30 €\n",
      "5. Thrustmaster Ferrari 458 Spider Racing Wheel 94,95 €\n",
      "5. Thrustmaster TMX Pro Racing Wheel Xbox One 219,94 €\n",
      "5. Destiny 2 Limited Edition Xbox One 109,99 €\n",
      "5. Destiny 2 Collector’s Edition Xbox One 249,99 €\n",
      "4. Forza Motosport 7 Ultimate Edition Xbox One 99,99 € 84,99 €\n",
      "5. Star Wars: Battlefront II: Elite Trooper Deluxe Edition Xbox One 89,99 €\n",
      "5. Dark Souls III Collector's Edition Xbox One 90 €\n",
      "5. Syberia 3 Collector's Edition Xbox One 129,99 €\n",
      "5. Stardew Valley Collector's Edition XBox One 30,99 €\n",
      "5. Deus Ex: Mankind Divided Collector's Edition Xbox One 89 €\n",
      "5. Mafia III Collector's Edition Xbox One 159,99 €\n",
      "5. Hitman Collector's Edition Xbox One 122 €\n",
      "5. Tekken 7 Collector's Edition Xbox One 152,99 €\n",
      "5. Rocket League Collector's Edition Xbox One 29,95 €\n",
      "5. Halo 5: Guardians Limited Collector's Edition Xbox One 199,99 €\n",
      "5. Destiny 2 Collector’s Edition Xbox One 249,99 €\n",
      "5. Assassin's Creed Origins Gods Collector's Edition Xbox One 129,99 €\n",
      "5. Project C.A.R.S. 2 Collector's Edition Xbox One 159,99 €\n",
      "5. South Park: The Fractured But Whole Collector's Edition Xbox One 99,99 €\n",
      "5. Dragon Ball: Xenoverse 2 Collector's Edition Xbox One 159,99 €\n",
      "5. Metal Gear Solid V: The Phantom Pain Collector's Edition Xbox One 129,99 €\n"
     ]
    }
   ],
   "source": [
    "             \n",
    "result_list = process_row_FNAC(df)               \n",
    "df_result = pd.DataFrame(result_list)\n",
    "df_result.columns = ['site', 'date', 'title', 'extractedData', 'linkToArchive', 'linkToExtractedText']\n",
    "\n",
    "df_result.to_csv('products_extracted_prices_FNAC.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
